__ドキュメント更新中のため記述に誤りがあるかもしれません。__

# 学習について、共通編
# 학습에 대해서, 공통편

본 레포트에서는 모델의 fine tuning, DreamBooth, 그리고 LoRA 및 Textual Inversion([XTI:P+](링크 참조, PromptPlus))의 학습을 서포트합니다. 이 문서에서는 학습 데이터의 준비 방법이나 옵션 등에 대해 설명합니다.
当リポジトリではモデルのfine tuning、DreamBooth、およびLoRAとTextual Inversion（[XTI:P+](https://github.com/kohya-ss/sd-scripts/pull/327)を含む）の学習をサポートします。この文書ではそれらに共通する、学習データの準備方法やオプション等について説明します。

# 개요
# 概要

본 문서에서는 다음과 같은 내용을 설명합니다.
あらかじめこのリポジトリのREADMEを参照し、環境整備を行ってください。

이하에 대해서 설명합니다.
以下について説明します。


1. 학습 데이터의 준비에 대해서(설정 파일을 이용하는 새로운 형식)
2. 학습에 사용되는 용어에 대해서 간단히 설명
3. 이전의 지정 형식(설정 파일을 이용하지 않고 커맨드 라인에서 지정)
4. 학습 중의 샘플 이미지 생성
5. 각 스크립트에서 공통으로 사용되는, 자주 사용되는 옵션
6. fine tuning 방식의 메타 데이터 준비: 캡셔닝 등

1. 学習データの準備について（設定ファイルを用いる新形式）
1. 学習で使われる用語のごく簡単な解説
1. 以前の指定形式（設定ファイルを用いずコマンドラインから指定）
1. 学習途中のサンプル画像生成
1. 各スクリプトで共通の、よく使われるオプション
1. fine tuning 方式のメタデータ準備：キャプションニングなど

우선 1.만 실행하면 일단 학습은 가능합니다(학습에 대해서는 각 스크립트의 문서를 참조). 2.이후에는 필요에 따라 참조하십시오.
1.だけ実行すればとりあえず学習は可能です（学習については各スクリプトのドキュメントを参照）。2.以降は必要に応じて参照してください。

# 학습 데이터의 준비에 대해서
# 学習データの準備について

학습 데이터는 임의의 폴더(복수도 가능)에 준비해 둡니다. 학습 데이터의 이미지 파일을 준비합니다. `.png`, `.jpg`, `.jpeg`, `.webp`, `.bmp` 를 지원합니다. 리사이즈 등의 전처리는 기본적으로 필요하지 않습니다.
任意のフォルダ（複数でも可）に学習データの画像ファイルを用意しておきます。`.png`, `.jpg`, `.jpeg`, `.webp`, `.bmp` をサポートします。リサイズなどの前処理は基本的に必要ありません。

그러나 학습 데이터의 이미지 크기가 너무 작으면 학습이 제대로 이루어지지 않을 수 있습니다. 최소한 128x128 이상의 크기를 권장합니다. 그러나 학습 해상도보다 작은 이미지는 사용하지 않거나, 미리 초고해상도 AI 등을 사용하여 확대해 두는 것을 권장합니다. 또한 너무 큰 이미지(3000x3000 픽셀 정도?)보다 큰 이미지는 오류가 발생할 수 있으므로 미리 축소해 주십시오.
ただし学習解像度（後述）よりも極端に小さい画像は使わないか、あらかじめ超解像AIなどで拡大しておくことをお勧めします。また極端に大きな画像（3000x3000ピクセル程度？）よりも大きな画像はエラーになる場合があるようですので事前に縮小してください。

학습시에는, 모델에 학습시킬 이미지 데이터를 정리하여, 스크립트에 지정해야 합니다. 학습 데이터의 수, 학습 대상, 캡셔닝(이미지의 설명)을 준비할 수 있는지 여부 등에 따라, 몇 가지 방법으로 학습 데이터를 지정할 수 있습니다. 다음과 같은 방식이 있습니다(각각의 이름은 일반적인 것이 아니라, 본 레포트의 고유한 정의입니다). 정규화 이미지에 대해서는 이후에 설명합니다.
学習時には、モデルに学ばせる画像データを整理し、スクリプトに対して指定する必要があります。学習データの数、学習対象、キャプション（画像の説明）が用意できるか否かなどにより、いくつかの方法で学習データを指定できます。以下の方式があります（それぞれの名前は一般的なものではなく、当リポジトリ独自の定義です）。正則化画像については後述します。

1. DreamBooth, class+identifier 방식(정규화 이미지 사용 가능)
1. DreamBooth、class+identifier方式（正則化画像使用可）

    각 이미지에 대해 특정한 단어(identifier)에 학습 대상을 묶어 학습합니다. 캡셔닝을 준비할 필요는 없습니다. 예를 들어 특정 캐릭터를 학습시키는 경우에 사용하면 캡셔닝을 준비할 필요가 없는 만큼, 간편하지만, 헤어스타일이나 의상, 배경 등 학습 데이터의 모든 요소가 identifier에 묶여 학습되기 때문에, 생성시의 프롬프트에서 옷을 바꿀 수 없는 등의 상황도 발생할 수 있습니다.
    特定の単語 (identifier) に学習対象を紐づけるように学習します。キャプションを用意する必要はありません。たとえば特定のキャラを学ばせる場合に使うとキャプションを用意する必要がない分、手軽ですが、髪型や服装、背景など学習データの全要素が identifier に紐づけられて学習されるため、生成時のプロンプトで服が変えられない、といった事態も起こりえます。

1. DreamBooth, 캡셔닝 방식(정규화 이미지 사용 가능)
1. DreamBooth、キャプション方式（正則化画像使用可）

    각 이미지에 대해 캡셔닝을 준비하여 학습합니다. 특정 캐릭터를 학습시키는 경우에는 캡셔닝에 캐릭터의 상세한 정보를 기록하는 것으로 (흰 옷을 입은 캐릭터A, 빨간 옷을 입은 캐릭터A 등) 캐릭터와 그 외의 요소가 분리되어, 더 엄격하게 캐릭터만을 학습할 수 있습니다.
    画像ごとにキャプションが記録されたテキストファイルを用意して学習します。たとえば特定のキャラを学ばせると、画像の詳細をキャプションに記述することで（白い服を着たキャラA、赤い服を着たキャラA、など）キャラとそれ以外の要素が分離され、より厳密にモデルがキャラだけを学ぶことが期待できます。

1. fine tuning 방식(정규화 이미지 사용 불가)
1. fine tuning方式（正則化画像使用不可）

    다시 한번 캡션을 메타데이터파일에 모아 정리합니다. 태그와 캡션을 나눠서 관리하거나, 학습을 빠르게 하기 위해 latents를 사전에 캐시하는 등의 기능을 지원합니다(둘 다 별도 문서에서 설명합니다). (fine tuning 방식이라는 이름이지만 fine tuning 방식만 사용할 수 있는 것은 아닙니다.)
    あらかじめキャプションをメタデータファイルにまとめます。タグとキャプションを分けて管理したり、学習を高速化するためlatentsを事前キャッシュしたりなどの機能をサポートします（いずれも別文書で説明しています）。（fine tuning方式という名前ですが fine tuning 以外でも使えます。）

학습하고자 하는 대상이나 방법에 따라, 학습하고자 하는 대상과 사용할 수 있는 지정 방식의 조합은 다음과 같습니다.
学習したいものと使用できる指定方法の組み合わせは以下の通りです。

| 학습 대상 또는 방법 | 스크립트 | DB / class+identifier | DB / 캡셔닝 | fine tuning |
| 学習対象または方法 | スクリプト | DB / class+identifier | DB / キャプション | fine tuning |
| ----- | ----- | ----- | ----- | ----- |
| 모델을 fine tuning | `fine_tune.py`| x | x | o |
| モデルをfine tuning | `fine_tune.py`| x | x | o |
| 모델을 DreamBooth fine tunijng| `train_db.py`| o | o | x |
| モデルをDreamBooth | `train_db.py`| o | o | x |
| LoRA | `train_network.py`| o | o | o |
| Textual Invesion | `train_textual_inversion.py`| o | o | o |

## 어떤 것을 선택할 것인가
## どれを選ぶか

LoRA, Textual Inversion에 대해서는, 캡셔닝 파일을 준비하지 않고 간편하게 학습하고 싶다면, DreamBooth class+identifier, 준비할 수 있다면 DreamBooth 캡셔닝 방식이 좋을 것입니다. 학습 데이터의 수가 많고, 정규화 이미지를 사용하지 않는 경우에는 fine tuning 방식도 고려해 보십시오.
LoRA、Textual Inversionについては、手軽にキャプションファイルを用意せずに学習したい場合はDreamBooth class+identifier、用意できるならDreamBooth キャプション方式がよいでしょう。学習データの枚数が多く、かつ正則化画像を使用しない場合はfine tuning方式も検討してください。

Dreambooth를 사용할 경우 fine tuning은 동시에 사용 불가능합니다. fine tuning의 경우, 반대의 경우 역시 불가능하며 fine tuning만을 사용할 수 있습니다.(데이터셋)
DreamBoothについても同様ですが、fine tuning方式は使えません。fine tuningの場合はfine tuning方式のみです。

# 각 방식의 지정 방법
# 各方式の指定方法について

이제부터는 각 방식의 지정 방법에 대해서 설명합니다. 각 방식의 지정 방법에 대해서는 각각의 문서를 참조하십시오.
ここではそれぞれの指定方法で典型的なパターンについてだけ説明します。より詳細な指定方法については [データセット設定](./config_README-ja.md) をご覧ください。

# DreamBooth, class+identifier 방식(정규화 이미지 사용 가능)
# DreamBooth、class+identifier方式（正則化画像使用可）

이 방식에서는 각 이미지는 `class identifier` 라는 캡셔닝으로 학습됩니다(`shs dog` 등). 이 방식에서는 각 이미지는 `class identifier` 라는 캡셔닝으로 학습됩니다(`shs dog` 등).
この方式では、各画像は `class identifier` というキャプションで学習されたのと同じことになります（`shs dog` など）。

## step 1. identifier와 class를 결정합니다
## step 1. identifierとclassを決める

학습시키고자 하는 대상을 identifier와 class로 결정합니다.
学ばせたい対象を結びつける単語identifierと、対象の属するclassを決めます。

(instance 등 여러 부르는 방법이 존재하지만, 우선 원본 논문의 표기법을 따릅니다.)
（instanceなどいろいろな呼び方がありますが、とりあえず元の論文に合わせます。）

이하 극히 간단히 설명한다면 (자세한 건 논문을 참조해 주십시오)
以下ごく簡単に説明します（詳しくは調べてください）。

class는 학습하고자 하는 대상의 일반적인 종별(class)입니다. 예를 들어, 특정 견종을 학습시키고자 할 경우, class는 dog입니다. 애니메이션 캐릭터의 경우 boy, girl, 1boy, 1girl이 되겠습니다.
classは学習対象の一般的な種別です。たとえば特定の犬種を学ばせる場合には、classはdogになります。アニメキャラならモデルによりboyやgirl、1boyや1girlになるでしょう。

identifier는 학습대상을 식별하기 위한 것입니다. 임의의 단어를 사용해도 상관 없지만, 원본 논문에 따르면 "tokinizer로 1토큰이 되는 3글자 이하의 레어한 단어"가 좋다고 합니다.
identifierは学習対象を識別して学習するためのものです。任意の単語で構いませんが、元論文によると「tokinizerで1トークンになる3文字以下でレアな単語」が良いとのことです。

identifier 및 class를 사용하면, 예를 들어 `shs dog` 등으로 모델을 학습시키면, 학습하고자 하는 대상을 class로 식별하여 학습시킬 수 있습니다.
identifierとclassを使い、たとえば「shs dog」などでモデルを学習することで、学習させたい対象をclassから識別して学習できます。

이미지 재생성시에는 `shs dog` 등으로 학습한 견종의 이미지가 생성됩니다.
画像生成時には「shs dog」とすれば学ばせた犬種の画像が生成されます。

(identifier로는 최근 참고한 것에 따르면, `shs sts scs cpc coc cic msm usu ici lvl cic dii muk ori hru rik koo yos wny` 등이 있습니다. 실제로는 Danbooru Tag에 포함되지 않는 것이 바람직합니다. 이와 같은 토큰은 1토큰이 되는 3글자 이하의 레어한 단어이기 때문입니다.)
（identifierとして私が最近使っているものを参考までに挙げると、``shs sts scs cpc coc cic msm usu ici lvl cic dii muk ori hru rik koo yos wny`` などです。本当は Danbooru Tag に含まれないやつがより望ましいです。）

## step 2. 정규화 이미지를 사용할 것인지 결정하고, 사용할 경우 정규화 이미지를 생성합니다
## step 2. 正則化画像を使うか否かを決め、使う場合には正則化画像を生成する

정규화 이미지란, 전에 설명한 class 전체가 학습 대상에 과적합되지 않게 하기 위한 이미지를 의미합니다.(이를 language drift, knowledge drift라 합니다.) 정규화영상을 사용하지 않으면, 예를 들어 `shs 1girl`로 특정 캐릭터를 학습시키면, 단순히 `1girl`이라는 프롬프트로 생성해도 그 캐릭터와 비슷해집니다. 이는 `1girl`이 학습시의 캡션이 포함되어 있기 때문입니다.
正則化画像とは、前述のclass全体が、学習対象に引っ張られることを防ぐための画像です（language drift）。正則化画像を使わないと、たとえば `shs 1girl` で特定のキャラクタを学ばせると、単なる `1girl` というプロンプトで生成してもそのキャラに似てきます。これは `1girl` が学習時のキャプションに含まれているためです。

학습대상의 이미지와 정규화 이미지를 동시에 학습시키면, class는 class 그대로 유지되고, identifier를 프롬프트에 붙였을 때에만 학습대상이 생성되도록 학습됩니다.
学習対象の画像と正則化画像を同時に学ばせることで、class は class のままで留まり、identifier をプロンプトにつけた時だけ学習対象が生成されるようになります。

LoRA, DreamBooth에서는 특정 캐릭터만 나오면 되는 경우에는 정규화 이미지를 사용하지 않아도 됩니다.
LoRAやDreamBoothで特定のキャラだけ出てくればよい場合は、正則化画像を用いなくても良いといえます。

Textual Inversion에서는 사용하지 않아도 됩니다(학습시키는 토큰 문자열이 캡션이 포함되지 않기 때문에 아무것도 학습되지 않습니다).
Textual Inversionでは用いなくてよいでしょう（学ばせる token string がキャプションに含まれない場合はなにも学習されないため）。

정규화 이미지는 학습 대상의 모델에서, class 이름만으로 생성한 이미지를 사용하는 것이 일반적입니다(예를 들어 `1girl` 등). 그러나 생성 이미지의 품질이 나쁜 경우에는 프롬프트를 고안하거나, 네트워크에서 별도로 다운로드한 이미지를 사용할 수도 있습니다.
正則化画像としては、学習対象のモデルで、class 名だけで生成した画像を用いるのが一般的です（たとえば `1girl`）。ただし生成画像の品質が悪い場合には、プロンプトを工夫したり、ネットから別途ダウンロードした画像を用いることもできます。

(정규화 이미지 또한 학습에 관여하기에, 이미지의 품질 및 데이터셋 자체는 모델에 영향을 미칩니다.)
（正則化画像も学習されるため、その品質はモデルに影響します。）

일반적으로 수백장 정도 준비하는 것이 바람직합니다(수가 적으면 class 이미지가 일반화되어 그들의 특징을 학습해 버립니다).
一般的には数百枚程度、用意するのが望ましいようです（枚数が少ないと class 画像が一般化されずそれらの特徴を学んでしまいます）。

생성 이미지를 사용할 경우, 일반적으로 생성 이미지의 크기는 학습 해상도(더 정확하게는 bucket의 해상도, 이후 설명)에 맞추는 것이 바람직합니다.
生成画像を使う場合、通常、生成画像のサイズは学習解像度（より正確にはbucketの解像度、後述）にあわせてください。

## step 2. 설정 파일의 기술
## step 2. 設定ファイルの記述

텍스트 파일을 작성하고, 확장자를 `.toml`로 합니다. 예를 들어 다음과 같이 기술합니다.
テキストファイルを作成し、拡張子を `.toml` にします。たとえば以下のように記述します。


(이하 `#`으로 시작하는 부분은 주석입니다. 이대로 복사하여 그대로 사용해도 좋고, 삭제해도 문제 없습니다.)
（`#` で始まっている部分はコメントですので、このままコピペしてそのままでもよいですし、削除しても問題ありません。）

```toml
[general]
enable_bucket = true                        # Aspect Ratio Bucketingを使うか否か # Aspect Ratio Bucketing을 사용할지

[[datasets]]
resolution = 512                            # 学習解像度 # 학습해상도
batch_size = 4                              # バッチサイズ # 배치사이즈

  [[datasets.subsets]]
  image_dir = 'C:\hoge'                     # 学習用画像を入れたフォルダを指定 # 학습용 이미지가 들어있는 폴더를 지정
  class_tokens = 'hoge girl'                # identifier class を指定 # identifier class를 지정
  num_repeats = 10                          # 学習用画像の繰り返し回数 # 학습용 이미지의 반복횟수

  # 以下は正則化画像を用いる場合のみ記述する。用いない場合は削除する # 이하는 정규화 이미지의 경우에만 사용됩니다. 쓰지 않을 경우 삭제합니다。
  [[datasets.subsets]]
  is_reg = true # 정규화 이미지임을 지정하기 위해 넣어야 합니다/
  image_dir = 'C:\reg'                      # 正則化画像を入れたフォルダを指定 # 정규화 이미지가 들어있는 폴더를 지정
  class_tokens = 'girl'                     # class を指定 # class를 지정
  num_repeats = 1                           # 正則化画像の繰り返し回数、基本的には1でよい # 정규화 이미지의 반복횟수, 기본적으로 1로 합니다.
```

기본적으로 아래의 부분만을 수정하면 학습할 수 있습니다.
基本的には以下の場所のみ書き換えれば学習できます。

1. 학습해상도
1. 学習解像度
    
    수치 하나를 지정하면 정방형(`512`라면 512x512), 괄호와 쉼표로 구분하여 2개를 지정하면 가로×세로(`[512,768]`라면 512x768)가 됩니다. SD1.x 계열에서는 원래의 학습해상도는 512입니다. `[512,768]` 등의 큰 해상도를 지정하면 세로로 긴, 가로로 긴 이미지 생성시의 문제를 줄일 수 있을지도 모릅니다. SD2.x 768 계열에서는 `768`입니다.
    数値1つを指定すると正方形（`512`なら512x512）、鍵カッコカンマ区切りで2つ指定すると横×縦（`[512,768]`なら512x768）になります。SD1.x系ではもともとの学習解像度は512です。`[512,768]` 等の大きめの解像度を指定すると縦長、横長画像生成時の破綻を小さくできるかもしれません。SD2.x 768系では `768` です。

1. Batch size
1. バッチサイズ
    동시에 몇 개의 데이터를 학습할지를 지정합니다. GPU의 VRAM 크기, 학습해상도에 따라 달라집니다. 자세한 것은 이후 설명합니다. 또한 fine tuning/DreamBooth/LoRA 등에서도 달라집니다. 각 스크립트의 설명도 참조해 주십시오.
    同時に何件のデータを学習するかを指定します。GPUのVRAMサイズ、学習解像度によって変わってきます。詳しくは後述します。またfine tuning/DreamBooth/LoRA等でも変わってきますので各スクリプトの説明もご覧ください。

1. 학습용 이미지 폴더 지정
1. フォルダ指定
    학습용 이미지, 정규화 이미지(사용할 경우에만)의 폴더를 지정합니다. 이미지 데이터가 포함된 폴더 자체를 지정합니다.
    学習用画像、正則化画像（使用する場合のみ）のフォルダを指定します。画像データが含まれているフォルダそのものを指定します。

1. identifier와 class 지정
1. identifier と class の指定
    앞서 설명한 대로 identifier와 class를 지정합니다.
    前述のサンプルの通りです。

1. 반복 횟수
1. 繰り返し回数
    후술합니다.
    後述します。

### 반복 횟수(Repeat number)에 대해서
### 繰り返し回数について

반복 횟수는, 정규화 이미지의 수와 학습용 이미지의 수를 조정하기 위해 사용됩니다. 정규화 이미지의 수는 학습용 이미지보다 많기 때문에, 학습용 이미지를 반복하여 수를 맞추어, 1대1의 비율로 학습할 수 있도록 합시다.
繰り返し回数は、正則化画像の枚数と学習用画像の枚数を調整するために用いられます。正則化画像の枚数は学習用画像よりも多いため、学習用画像を繰り返して枚数を合わせ、1対1の比率で学習できるようにします。

반복 횟수는 학습용 이미지의 반복 횟수 x 학습용 이미지의 갯수 >= 정규화 이미지의 반복 횟수 x 정규화 이미지의 갯수 가 되도록 지정하는 것을 추천합니다.
繰り返し回数は「 __学習用画像の繰り返し回数×学習用画像の枚数≧正則化画像の繰り返し回数×正則化画像の枚数__ 」となるように指定してください。

(1에포크 (데이터가 한바퀴 도는 것을 1에포크라고 합니다)의 데이터 수는 "학습용 이미지의 반복 횟수 x 학습용 이미지의 갯수"가 됩니다. 정규화 이미지의 갯수가 그보다 많으면, 남은 정규화 이미지는 사용되지 않습니다.)
（1 epoch（データが一周すると1 epoch）のデータ数が「学習用画像の繰り返し回数×学習用画像の枚数」となります。正則化画像の枚数がそれより多いと、余った部分の正則化画像は使用されません。）

## step 3. 학습
## step 3. 学習

각각의 문서를 참조하십시오.
それぞれのドキュメントを参考に学習を行ってください。

# DreamBooth, 캡셔닝 방식(정규화 이미지 사용 가능)
# DreamBooth、キャプション方式（正則化画像使用可）

이 방식에서는 각 이미지는 캡셔닝으로 학습됩니다.
この方式では各画像はキャプションで学習されます。

## step 1. 캡셔닝 파일을 준비합니다
## step 1. キャプションファイルを準備する

학습용 이미지 폴더에, 같은 파일명으로 `.caption` 확장자(설정에서 변경할 수 있습니다)의 파일을 두어 주십시오. 각 파일은 1행만을 기록합니다. 인코딩은 `UTF-8`입니다.
学習用画像のフォルダに、画像と同じファイル名で、拡張子 `.caption`（設定で変えられます）のファイルを置いてください。それぞれのファイルは1行のみとしてください。エンコーディングは `UTF-8` です。

## step 2. 정규화 이미지를 사용할 것인지 결정하고, 사용할 경우 정규화 이미지를 생성합니다. (스스로 하세요!)
## step 2. 正則化画像を使うか否かを決め、使う場合には正則化画像を生成する

class+identifier 방식과 동일합니다. 정규화 이미지를 사용하지 않는 경우에는 이 단계를 생략합니다.
class+identifier形式と同様です。なお正則化画像にもキャプションを付けることができますが、通常は不要でしょう。

## step 2. 설정 파일의 기술
## step 2. 設定ファイルの記述

텍스트 파일을 작성하고, 확장자를 `.toml`로 합니다. 예를 들어 다음과 같이 기술합니다.
テキストファイルを作成し、拡張子を `.toml` にします。たとえば以下のように記述します。

```toml
[general]
enable_bucket = true                        # Aspect Ratio Bucketingを使うか否か # Aspect Ratio Bucketing을 사용할지 여부

[[datasets]]
resolution = 512                            # 学習解像度 # 학습해상도
batch_size = 4                              # バッチサイズ # 배치사이즈

  [[datasets.subsets]]
  image_dir = 'C:\hoge'                     # 学習用画像を入れたフォルダを指定 # 학습용 이미지가 들어있는 폴더를 지정
  caption_extension = '.caption'            # キャプションファイルの拡張子　.txt を使う場合には書き換える # 캡셔닝 파일의 확장자, .txt를 사용할 경우에는 변경합니다.
  num_repeats = 10                          # 学習用画像の繰り返し回数 # 학습용 이미지의 반복횟수

  # 以下は正則化画像を用いる場合のみ記述する。用いない場合は削除する # 이하는 정규화 이미지의 경우에만 사용됩니다. 쓰지 않을 경우 삭제합니다。
  [[datasets.subsets]]
  is_reg = true
  image_dir = 'C:\reg'                      # 正則化画像を入れたフォルダを指定 # 정규화 이미지가 들어있는 폴더를 지정
  class_tokens = 'girl'                     # class を指定 # class를 지정
  num_repeats = 1                           # 正則化画像の繰り返し回数、基本的には1でよい # 정규화 이미지의 반복횟수, 기본적으로 1로 합니다.
```

기본적으로는 이하의 사항들을 수정하면 됩니다. 특히 기술하지 않은 부분은, class + identifier와 동일하게 작성하면 됩니다.
基本的には以下を場所のみ書き換えれば学習できます。特に記述がない部分は class+identifier 方式と同じです。
1. 학습 해상도
1. 배치 사이즈
1. 폴더명 지정
1. 캡션 확장자
1. 반복 횟수

1. 学習解像度
1. バッチサイズ
1. フォルダ指定
1. キャプションファイルの拡張子

    任意の拡張子を指定できます。
1. 繰り返し回数

## step 3. 학습
## step 3. 学習

각각의 문서를 참조하십시오.
それぞれのドキュメントを参考に学習を行ってください。

# fine tuning 방식
# fine tuning 方式

## step 1. 메타데이터를 준비합니다
## step 1. メタデータを準備する

캡션이나 태그를 모아 관리하는 파일을 메타데이터라고 합니다. json 형식이며, 확장자는 `.json`입니다. 작성 방법은 길어지므로 이 문서의 맨 끝에 작성했습니다.
キャプションやタグをまとめた管理用ファイルをメタデータと呼びます。json形式で拡張子は `.json`
 です。作成方法は長くなりますのでこの文書の末尾に書きました。

## step 2. 설정 파일의 기술
## step 2. 設定ファイルの記述

텍스트 파일을 작성하고, 확장자를 `.toml`로 합니다. 예를 들어 다음과 같이 기술합니다.
テキストファイルを作成し、拡張子を `.toml` にします。たとえば以下のように記述します。

```toml
[general]
shuffle_caption = true
keep_tokens = 1

[[datasets]]
resolution = 512                                    # 学習解像度 # 학습해상도
batch_size = 4                                      # バッチサイズ # 배치사이즈

  [[datasets.subsets]]
  image_dir = 'C:\piyo'                             # 学習用画像を入れたフォルダを指定 # 학습용 이미지가 들어있는 폴더를 지정
  metadata_file = 'C:\piyo\piyo_md.json'            # メタデータファイル名 # 메타데이터 파일명
```

기본적으로는 이하의 사항들을 수정하면 됩니다. 특히 기술하지 않은 부분은, class + identifier와 동일하게 작성하면 됩니다.
基本的には以下を場所のみ書き換えれば学習できます。特に記述がない部分は DreamBooth, class+identifier 方式と同じです。

1. 학습 해상도
1. 배치 사이즈
1. 폴더명 지정
1. 메타데이터 파일명

    앞서 작성한 메타데이터 파일을 지정합니다.
1. 学習解像度
1. バッチサイズ
1. フォルダ指定
1. メタデータファイル名

    後述の方法で作成したメタデータファイルを指定します。


## step 3. 학습
## step 3. 学習

각각의 문서를 참조하십시오.
それぞれのドキュメントを参考に学習を行ってください。

# 학습에 사용되는 관련 용어의 간단한 해설
# 学習で使われる用語のごく簡単な解説

자세한 것은 생략하고, 저도 모르는 부분이 있어서 더 자세한건 스스로 찾아보시기 바랍니다.
細かいことは省略していますし私も完全には理解していないため、詳しくは各自お調べください。

## fine tuning
## fine tuning（ファインチューニング）

모델을 학습해서 미세 조정하는 것을 지칭합니다. 사용 방법에 따라 의미가 다르지만, 좁은 의미의 fine tuning은 Stable Diffusion의 경우, 모델을 이미지와 캡션이 함께 학습하는 것을 의미합니다. DreamBooth는 좁은 의미의 fine tuning의 특수한 방식이라고 할 수 있습니다. 넓은 의미의 fine tuning은 LoRA, Textual Inversion, Hypernetworks 등을 포함하며, 모델을 학습하는 것을 모두 포함합니다.
モデルを学習して微調整することを指します。使われ方によって意味が異なってきますが、狭義のfine tuningはStable Diffusionの場合、モデルを画像とキャプションで学習することです。DreamBoothは狭義のfine tuningのひとつの特殊なやり方と言えます。広義のfine tuningは、LoRAやTextual Inversion、Hypernetworksなどを含み、モデルを学習することすべてを含みます。

## 스텝
## ステップ

간단히 말하면 학습 데이터로 1회 계산하면 1스텝입니다. "학습 데이터의 캡션이 현재 모델에 흘려보고, 나오는 이미지를 학습 데이터의 이미지와 비교하여, 학습 데이터에 가까워지도록 모델을 약간 변경한다"가 1스텝입니다.
ざっくりいうと学習データで1回計算すると1ステップです。「学習データのキャプションを今のモデルに流してみて、出てくる画像を学習データの画像と比較し、学習データに近づくようにモデルをわずかに変更する」のが1ステップです。

## 배치 사이즈
## バッチサイズ

배치 사이즈란, 1스텝에 몇 건의 데이터를 한꺼번에 계산할지를 지정하는 값입니다. 한꺼번에 계산하므로 속도는 상대적으로 향상합니다. 또한 일반적으로는 정확도도 높아집니다.
バッチサイズは1ステップで何件のデータをまとめて計算するかを指定する値です。まとめて計算するため速度は相対的に向上します。また一般的には精度も高くなるといわれています。

'배치 사이즈 x 스텝 수'가 학습에 사용되는 데이터 건수가 됩니다. 그러므로 배치 사이즈를 늘리면 스텝 수를 줄여야 합니다.

`バッチサイズ×ステップ数` が学習に使われるデータの件数になります。そのため、バッチサイズを増やした分だけステップ数を減らすとよいでしょう。

(단, '배치 사이즈 1로 1600스텝'과 '배치 사이즈 4로 400스텝'은 같은 결과가 나오지 않습니다. 같은 학습률의 경우, 일반적으로는 후자가 학습 미흡, 즉 언더피팅에 빠집니다. 학습률을 조금 높이거나(예를 들어 `2e-6` 등), 스텝 수를 500스텝 등으로 조정해 주세요.)
（ただし、たとえば「バッチサイズ1で1600ステップ」と「バッチサイズ4で400ステップ」は同じ結果にはなりません。同じ学習率の場合、一般的には後者のほうが学習不足になります。学習率を多少大きくするか（たとえば `2e-6` など）、ステップ数をたとえば500ステップにするなどして工夫してください。）

배치 사이즈를 크게 하면 그만큼 GPU 메모리를 소비합니다. 메모리가 부족하면 에러가 발생하고, 에러가 발생하지 않더라도 학습 속도가 느려집니다. 태스크 매니저나 `nvidia-smi` 명령으로 사용 메모리 양을 확인하면서 조정하면 좋습니다.
バッチサイズを大きくするとその分だけGPUメモリを消費します。メモリが足りなくなるとエラーになりますし、エラーにならないギリギリでは学習速度が低下します。タスクマネージャーや `nvidia-smi` コマンドで使用メモリ量を確認しながら調整するとよいでしょう。

또한, 배치는 '한 묶음의 데이터'와 같은 의미입니다.
なお、バッチは「一塊のデータ」位の意味です。


## 학습률
## 学習率

간단히 말해, 1스텝당 어느 정도의 변화를 주는지를 지정합니다. 큰 값을 지정하면 그만큼 빠르게 학습이 진행되지만, 너무 크면 모델이 망가지거나, 최적 상태에 도달하지 못할 수 있습니다. 작은 값을 지정하면 학습 속도는 느려지지만, 역시 최적 상태에 도달하지 못할 수 있습니다.
ざっくりいうと1ステップごとにどのくらい変化させるかを表します。大きな値を指定するとそれだけ速く学習が進みますが、変化しすぎてモデルが壊れたり、最適な状態にまで至れない場合があります。小さい値を指定すると学習速度は遅くなり、また最適な状態にやはり至れない場合があります。

fine tuning, DreamBooth, LoRA 각각의 경우, 학습률의 기본값이 다릅니다. 또한 학습 데이터나 학습하고자 하는 모델, 배치 사이즈, 스텝 수에 따라 달라집니다. 일반적인 값에서 시작하여, 학습 상태를 보면서 조금씩 늘리거나 줄여가면서 찾아가는 것이 좋습니다.
fine tuning、DreamBoooth、LoRAそれぞれで大きく異なり、また学習データや学習させたいモデル、バッチサイズやステップ数によっても変わってきます。一般的な値から初めて学習状態を見ながら増減してください。

디폴트로는 학습 전반에 걸쳐 학습률을 고정하게 되어있습니다. 스케줄러를 지정하여 학습률을 어떻게 변화시킬지를 지정할 수 있습니다. 이것에 따라 결과가 달라집니다.
デフォルトでは学習全体を通して学習率は固定です。スケジューラの指定で学習率をどう変化させるか決められますので、それらによっても結果は変わってきます。

## 에포크
## エポック（epoch）

학습 데이터가 한 바퀴 도는 것을 1에포크라고 합니다. 반복 횟수를 지정한 경우, 그 반복 후의 데이터가 한 바퀴 도는 것을 1에포크라고 합니다.
学習データが一通り学習されると（データが一周すると）1 epochです。繰り返し回数を指定した場合は、その繰り返し後のデータが一周すると1 epochです。

1에포크의 스텝 수는 기본적으로 '학습 데이터의 수 / 배치 사이즈'입니다. 그러나 Aspect Ratio Bucketing을 사용하는 경우, 약간 늘어납니다(다른 버킷의 데이터는 같은 배치에 넣을 수 없으므로, 스텝 수가 늘어납니다).
1 epochのステップ数は、基本的には `データ件数÷バッチサイズ` ですが、Aspect Ratio Bucketing を使うと微妙に増えます（異なるbucketのデータは同じバッチにできないため、ステップ数が増えます）。

## Aspect Ratio Bucketing

Stable Diffusion v1은 512\*512에서 학습되었지만, 이에 더해 256\*1024, 384\*640 등의 해상도에서도 학습합니다. 이를 통해 트리밍되는 부분이 줄어들어, 더 정확하게 캡션과 이미지의 관계가 학습될 것으로 기대됩니다.
Stable Diffusion のv1は512\*512で学習されていますが、それに加えて256\*1024や384\*640といった解像度でも学習します。これによりトリミングされる部分が減り、より正しくキャプションと画像の関係が学習されることが期待されます。

또한 임의의 해상도에서 학습할 수 있으므로, 이미지 데이터의 가로 세로 비율을 미리 통일해 둘 필요가 없어집니다.
また任意の解像度で学習するため、事前に画像データの縦横比を統一しておく必要がなくなります。

이 기능은 설정에서 유효, 무효를 전환할 수 있습니다. 이 문서의 설정 파일 예시에서는 유효하게 설정되어 있습니다(`true`가 설정되어 있습니다).
設定で有効、無効が切り替えられますが、ここまでの設定ファイルの記述例では有効になっています（`true` が設定されています）。

이 기능을 사용하는 경우, 학습 해상도는 파라미터로 지정된 해상도의 면적(=메모리 사용량)을 넘지 않는 범위에서, 64픽셀 단위(디폴트, 변경 가능)로 세로 가로를 조정하여 생성됩니다.
学習解像度はパラメータとして与えられた解像度の面積（＝メモリ使用量）を超えない範囲で、64ピクセル単位（デフォルト、変更可）で縦横に調整、作成されます。

기계 학습에서는 입력 크기를 모두 통일하는 것이 일반적이지만, 특별한 제약은 없으며, 실제로는 같은 배치 내에서 통일되어 있으면 됩니다. NovelAI에서 말하는 bucketing은, 미리 학습 데이터를, 가로 세로 비율에 따른 학습 해상도별로 분류해 두는 것을 의미합니다. 그리고 배치를 각 bucket의 이미지로 구성하여, 배치의 이미지 크기를 통일합니다.
機械学習では入力サイズをすべて統一するのが一般的ですが、特に制約があるわけではなく、実際は同一のバッチ内で統一されていれば大丈夫です。NovelAIの言うbucketingは、あらかじめ教師データを、アスペクト比に応じた学習解像度ごとに分類しておくことを指しているようです。そしてバッチを各bucket内の画像で作成することで、バッチの画像サイズを統一します。

# 메타데이터 파일 작성 방법 (설정파일을 통하지 않고, CLI에서 지정)
# 以前の指定形式（設定ファイルを用いずコマンドラインから指定）

'.toml' 파일을 지정하지 않고, 커맨드 라인 옵션으로 지정하는 방법입니다. DreamBooth class+identifier 방식, DreamBooth 캡셔닝 방식, fine tuning 방식이 있습니다.
`.toml` ファイルを指定せずコマンドラインオプションで指定する方法です。DreamBooth class+identifier方式、DreamBooth キャプション方式、fine tuning方式があります。

## DreamBooth, class+identifier 방식
## DreamBooth、class+identifier方式

폴더명으로 반복 횟수 및 class를 지정합니다. `train_data_dir` 옵션과 `reg_data_dir` 옵션을 사용합니다.
フォルダ名で繰り返し回数を指定します。また `train_data_dir` オプションと `reg_data_dir` オプションを用います。

### step 1. 학습용 이미지 준비
### step 1. 学習用画像の準備

학습 이미지를 저장할 폴더를 만듭니다. 그리고 그 안에, 반복 횟수와 class를 지정하여 폴더를 만듭니다.

学習用画像を格納するフォルダを作成します。 __さらにその中に__ 、以下の名前でディレクトリを作成します。

```
<반복 횟수>_<identifier> <class>
<繰り返し回数>_<identifier> <class>
```

중간의 _는 빠뜨리지 않도록 주의해 주세요. 예를 들어 "sls frog"라는 프롬프트에서, 데이터를 20회 반복하는 경우, "20_sls frog"가 됩니다. 아래와 같습니다.
間の``_``を忘れないでください。

たとえば「sls frog」というプロンプトで、データを20回繰り返す場合、「20_sls frog」となります。以下のようになります。

![image](https://user-images.githubusercontent.com/52813779/210770636-1c851377-5936-4c15-90b7-8ac8ad6c2074.png)

### 복수 class, 복수 대상(identifier)의 학습
### 複数class、複数対象（identifier）の学習

방법은 단순히, 학습 이미지 폴더 내에 ``반복 횟수_<identifier> <class>``의 폴더를 여러 개, 정규화 이미지 폴더에도 동일하게 ``반복 횟수_<class>``의 폴더를 여러 개 준비하면 됩니다.
方法は単純で、学習用画像のフォルダ内に ``繰り返し回数_<identifier> <class>`` のフォルダを複数、正則化画像フォルダにも同様に ``繰り返し回数_<class>`` のフォルダを複数、用意してください。

예를 들어 "sls frog"와 "cpc rabbit"를 동시에 학습하는 경우, 다음과 같습니다.
たとえば「sls frog」と「cpc rabbit」を同時に学習する場合、以下のようになります。

![image](https://user-images.githubusercontent.com/52813779/210777933-a22229db-b219-4cd8-83ca-e87320fc4192.png)

class가 하나로, 대상이 여럿일 경우, 정규화 이미지는  하나만 있어도 됩니다. 예를 들어 1girl에 캐릭터 A와 캐릭터 B가 있는 경우, 다음과 같이 합니다.
classがひとつで対象が複数の場合、正則化画像フォルダはひとつで構いません。たとえば1girlにキャラAとキャラBがいる場合は次のようにします。

- train_girls
  - 10_sls 1girl
  - 10_cpc 1girl
- reg_girls
  - 1_1girl

### step 2. 정규화 이미지 준비
### step 2. 正則化画像の準備

정규화 이미지를 사용하기 위한 준비입니다.
正則化画像を使う場合の手順です。

정규화 이미지를 저장할 폴더를 만듭니다. 그리고 그 안에, 반복 횟수와 class를 지정하여 폴더를 만듭니다. 바로 그 안에 정규화 이미지를 넣습니다.
正則化画像を格納するフォルダを作成します。 __さらにその中に__  ``<繰り返し回数>_<class>`` という名前でディレクトリを作成します。

예를 들어 'frog'라는 프롬프트에서, 데이터를 1회 반복하는 경우, 다음과 같습니다.
たとえば「frog」というプロンプトで、データを繰り返さない（1回だけ）場合、以下のようになります。

![image](https://user-images.githubusercontent.com/52813779/210770897-329758e5-3675-49f1-b345-c135f1725832.png)


### step 3. 학습 실행
### step 3. 学習の実行

각 학습 스크립트를 실행해주세요. `--train_data_dir` 옵션으로 앞서 만든 학습 이미지 폴더를, `--reg_data_dir` 옵션으로 정규화 이미지 폴더를 지정해 주세요.
各学習スクリプトを実行します。 `--train_data_dir` オプションで前述の学習用データのフォルダを（__画像を含むフォルダではなく、その親フォルダ__）、`--reg_data_dir` オプションで正則化画像のフォルダ（__画像を含むフォルダではなく、その親フォルダ__）を指定してください。

## DreamBooth, 캡셔닝 방식
## DreamBooth、キャプション方式

학습용 이미지, 정규화 이미지 폴더에, 각 이미지와 같은 파일명의, 확장자를 '.caption'으로 한 파일을 두면, 이 파일이 캡션용으로 사용됩니다.
学習用画像、正則化画像のフォルダに、画像と同じファイル名で、拡張子.caption（オプションで変えられます）のファイルを置くと、そのファイルからキャプションを読み込みプロンプトとして学習します。

※그러한 이미지의 경우, 폴더명은 사용되지 않게 됩니다. 주의해 주세요.
※それらの画像の学習に、フォルダ名（identifier class）は使用されなくなります。

캡션 파일의 확장자는 디폴트로 .caption입니다. 학습 스크립트의 `--caption_extension` 옵션으로 변경할 수 있습니다. `--shuffle_caption` 옵션으로 학습 시 캡션에 대해, 콤마로 구분된 각 부분을 셔플하면서 학습합니다.
キャプションファイルの拡張子はデフォルトで.captionです。学習スクリプトの `--caption_extension` オプションで変更できます。`--shuffle_caption` オプションで学習時のキャプションについて、カンマ区切りの各部分をシャッフルしながら学習します。

## fine tuning 방식
## fine tuning 方式

메타데잍터를 만드는 것까지는 설정 파일을 사용하는 방식과 동일합니다. `in_json` 옵션으로 메타데이터 파일을 지정합니다.
メタデータを作るところまでは設定ファイルを使う場合と同様です。`in_json` オプションでメタデータファイルを指定します。

# 학습 도중에서 샘플 출력
# 学習途中でのサンプル出力

학습중의 모델을 테스트하고 영상을 생성해 볼 수 있습니다. 학습 스크립트에 다음의 옵션을 지정합니다.
学習中のモデルで試しに画像生成することで学習の進み方を確認できます。学習スクリプトに以下のオプションを指定します。

- `--sample_every_n_steps` / `--sample_every_n_epochs`
    샘플을 출력할 스텝 수 또는 에포크 수를 지정합니다. 이 수마다 샘플을 출력합니다. 둘 다 지정하는 경우, 에포크 수가 우선됩니다.
    サンプル出力するステップ数またはエポック数を指定します。この数ごとにサンプル出力します。両方指定するとエポック数が優先されます。

- `--sample_prompts`
    샘플 출력용 프롬프트의 파일을 지정합니다.
    サンプル出力用プロンプトのファイルを指定します。

- `--sample_sampler`
    샘플 출력에 사용할 샘플러를 지정합니다.
    `'ddim', 'pndm', 'heun', 'dpmsolver', 'dpmsolver++', 'dpmsingle', 'k_lms', 'k_euler', 'k_euler_a', 'k_dpm_2', 'k_dpm_2_a'`가 선택 가능합니다.
    サンプル出力に使うサンプラーを指定します。
    `'ddim', 'pndm', 'heun', 'dpmsolver', 'dpmsolver++', 'dpmsingle', 'k_lms', 'k_euler', 'k_euler_a', 'k_dpm_2', 'k_dpm_2_a'`が選べます。

샘플을 출력할 때는, 먼저 샘플 출력용 프롬프트의 파일을 지정합니다. 이 파일에는 샘플 출력용 프롬프트를 한 줄에 하나씩 적어 둡니다. 여러 줄을 적어 두면, 각 줄마다 샘플이 출력됩니다.
サンプル出力を行うにはあらかじめプロンプトを記述したテキストファイルを用意しておく必要があります。1行につき1プロンプトで記述します。

예를 들어, 다음과 같은 파일을 만들어 둡니다.
たとえば以下のようになります。

```txt
# prompt 1
masterpiece, best quality, 1girl, in white shirts, upper body, looking at viewer, simple background --n low quality, worst quality, bad anatomy,bad composition, poor, low effort --w 768 --h 768 --d 1 --l 7.5 --s 28

# prompt 2
masterpiece, best quality, 1boy, in business suit, standing at street, looking back --n low quality, worst quality, bad anatomy,bad composition, poor, low effort --w 576 --h 832 --d 2 --l 5.5 --s 40
```

서두가 `#`일 경우 주석이 됩니다. `--n`과 같이 `--` + 영소문자로 시작하는 줄은, 샘플 출력에 사용할 옵션을 지정할 수 있습니다. 다음이 사용 가능합니다.
先頭が `#` の行はコメントになります。`--n` のように 「`--` + 英小文字」で生成画像へのオプションを指定できます。以下が使えます。

- `--n` 次のオプションまでをネガティブプロンプトとします。다음 옵션(또는 \n)까지 네거티브 프롬프트로 사용합니다.
- `--w` 生成画像の横幅を指定します。생성 이미지의 가로 크기를 지정합니다.
- `--h` 生成画像の高さを指定します。생성 이미지의 세로 크기를 지정합니다.
- `--d` 生成画像のseedを指定します。생성 이미지의 seed를 지정합니다.
- `--l` 生成画像のCFG scaleを指定します。생성 이미지의 CFG scale을 지정합니다.
- `--s` 生成時のステップ数を指定します。생성 시의 스텝 수를 지정합니다.

# 각 스크립트 공통에서 쓰이는 옵션
# 各スクリプトで共通の、よく使われるオプション

스크립트 갱신 시, 문서 갱신이 따라오지 못하는 경우가 있습니다. 그럴 경우 `--help` 옵션으로 사용 가능한 옵션을 확인해 주세요.
スクリプトの更新後、ドキュメントの更新が追い付いていない場合があります。その場合は `--help` オプションで使用できるオプションを確認してください。

## 학습에 사용하는 모델 지정
## 学習に使うモデル指定

- `--v2` / `--v_parameterization`
    
    학습 대상 모델은 일반적으로 v1이지만, Stable diffusion 2 base, 또는 v2 기반 모델의 경우 v2 옵션을, Stable diffusion 2 base, 768-v-ema.ckpt, 그리고 그들의 fine tuning 모델의 경우 v2와 v_parameterization 옵션을 함께 지정해 주세요.
    学習対象モデルとしてHugging Faceのstable-diffusion-2-base、またはそこからのfine tuningモデルを使う場合（推論時に `v2-inference.yaml` を使うように指示されているモデルの場合）は `--v2` オプションを、stable-diffusion-2や768-v-ema.ckpt、およびそれらのfine tuningモデルを使う場合（推論時に `v2-inference-v.yaml` を使うモデルの場合）は `--v2` と `--v_parameterization` の両方のオプションを指定してください。

    Stable Diffusion 2.0에서 크게 바뀐 이하의 점이 있습니다.
    Stable Diffusion 2.0では大きく以下の点が変わっています。

    1. 使用するTokenizer 사용하는 Tokenizer
    2. 使用するText Encoderおよび使用する出力層（2.0は最後から二番目の層を使う）사용할 Text encoder 및 사용할 출력 계층（2.0은 마지막에서 두 번째 계층을 사용합니다.）
    3. Text Encoderの出力次元数（768->1024） Text encoder의 출력 차원 수（768->1024）
    4. U-Netの構造（CrossAttentionのhead数など） U-Net의 구조（CrossAttention의 head 수 등）
    5. v-parameterization（サンプリング方法が変更されているらしい） v-parameterization（샘플링 방법이 변경되었다고 합니다.）

    이 중 base에서는 1~4가, base가 아닌 쪽(768-v)에서는 1~5가 사용됩니다. 1~4를 사용하는 것이 v2 옵션, 5를 사용하는 것이 v_parameterization 옵션입니다.
    このうちbaseでは1～4が、baseのつかない方（768-v）では1～5が採用されています。1～4を有効にするのがv2オプション、5を有効にするのがv_parameterizationオプションです。

    이러한 옵션을 v2가 아닌 이전 모델에서 사용할 경우 학습이 정상적으로 진행되지 않으니 주의해 주세요.

- `--pretrained_model_name_or_path` 
    
    추가학습을 진행할 모델을 지정합니다. Stable Diffusion의 checkpoint 파일(.ckpt 또는 .safetensors), Diffusers의 로컬 디스크에 있는 모델 디렉토리, Diffusers의 모델 ID("stabilityai/stable-diffusion-2" 등)를 지정할 수 있습니다.
    追加学習を行う元となるモデルを指定します。Stable Diffusionのcheckpointファイル（.ckptまたは.safetensors）、Diffusersのローカルディスクにあるモデルディレクトリ、DiffusersのモデルID（"stabilityai/stable-diffusion-2"など）が指定できます。

## 학습에 관한 설정들
## 学習に関する設定

- `--output_dir` 
    학습 후의 모델을 저장할 폴더를 지정합니다.
    学習後のモデルを保存するフォルダを指定します。
    
- `--output_name` 
    모델의 파일명을 지정합니다. 확장자는 제외하고 지정합니다.
    モデルのファイル名を拡張子を除いて指定します。
    
- `--dataset_config` 
    데이터셋의 설정을 기록한 .toml 파일을 지정합니다.
    データセットの設定を記述した `.toml` ファイルを指定します。

- `--max_train_steps` / `--max_train_epochs`
    학습할 스텝 수 또는 에포크 수를 지정합니다. 둘 다 지정하는 경우, 에포크 수가 우선됩니다.
    学習するステップ数やエポック数を指定します。両方指定するとエポック数のほうが優先されます。

- `--mixed_precision`
    메모리 절약을 위해 mixed precision(혼합 정밀도)로 학습합니다. `--mixed_precision="fp16"`과 같이 지정합니다. mixed precision 없이(디폴트)와 비교하여 정확도가 낮아질 수 있습니다만, 학습에 필요한 GPU 메모리 양이 크게 줄어듭니다.
    省メモリ化のため mixed precision （混合精度）で学習します。`--mixed_precision="fp16"` のように指定します。mixed precision なし（デフォルト）と比べて精度が低くなる可能性がありますが、学習に必要なGPUメモリ量が大きく減ります。
    (RTX 30시리즈 이후에서는 `bf16`도 지정할 수 있습니다. 환경 구축 시 accelerate에 사용한 설정과 맞춰주세요).
    （RTX30 シリーズ以降では `bf16` も指定できます。環境整備時にaccelerateに行った設定と合わせてください）。
    
- `--gradient_checkpointing`
    학습시의 가중치 계산을 한꺼번에 하지 않고 조금씩 하여, 학습에 필요한 GPU 메모리 양을 줄입니다. on/off는 정확도에 영향을 주지 않지만, on으로 하면 배치 사이즈를 크게 할 수 있어, 그쪽에서의 영향은 있습니다.
    学習時の重みの計算をまとめて行うのではなく少しずつ行うことで、学習に必要なGPUメモリ量を減らします。オンオフは精度には影響しませんが、オンにするとバッチサイズを大きくできるため、そちらでの影響はあります。
    또한 일반적으로는 on으로 하면 속도가 느려지지만, 배치 사이즈를 크게 할 수 있어, 전체적으로는 빨라질 수도 있습니다.
    また一般的にはオンにすると速度は低下しますが、バッチサイズを大きくできるので、トータルでの学習時間はむしろ速くなるかもしれません。

- `--xformers` / `--mem_eff_attn`
    xformers 옵션을 지정하면 xformers의 CrossAttention을 사용합니다. xformers를 설치하지 않았거나 에러가 발생하는 경우(환경에 따라 다릅니다만 `mixed_precision="no"`의 경우 등), 대신 `mem_eff_attn` 옵션을 지정하면 메모리 절약 버전의 CrossAttention을 사용합니다(xformers보다 속도는 느립니다).
    xformersオプションを指定するとxformersのCrossAttentionを用います。xformersをインストールしていない場合やエラーとなる場合（環境にもよりますが `mixed_precision="no"` の場合など）、代わりに `mem_eff_attn` オプションを指定すると省メモリ版CrossAttentionを使用します（xformersよりも速度は遅くなります）。

- `--clip_skip`
    `2`를 지정하면, Text Encoder (CLIP)의 뒤에서 두 번째 계층의 출력을 사용합니다. 1 또는 옵션을 생략하면 마지막 계층을 사용합니다.
    `2` を指定すると、Text Encoder (CLIP) の後ろから二番目の層の出力を用います。1またはオプション省略時は最後の層を用います。

    ※SD2.0은 디폴트로 뒤에서 두 번째 계층을 사용하므로, SD2.0의 학습에는 지정하면 안됩니다.
    ※SD2.0はデフォルトで後ろから二番目の層を使うため、SD2.0の学習では指定しないでください。

    학습 대상 모델이 원래 두 번째 계층을 사용하도록 학습되어 있는 경우, 2를 지정하면 좋습니다. 그렇지 않고 마지막 계층을 사용하도록 학습되어 있는 경우, 기반 모델 전체가 그것을 전제로 학습되어 있습니다. 그러므로 다시 두 번째 계층을 사용하여 학습하려면, 일정 수의 학습 데이터, 긴 학습 시간이 필요할 것입니다.
    学習対象のモデルがもともと二番目の層を使うように学習されている場合は、2を指定するとよいでしょう。

    そうではなく最後の層を使用していた場合はモデル全体がそれを前提に学習されています。そのため改めて二番目の層を使用して学習すると、望ましい学習結果を得るにはある程度の枚数の教師データ、長めの学習が必要になるかもしれません。

- `--max_token_length`
    디폴트는 75입니다. `150` 또는 `225`를 지정하면 토큰 길이를 확장하여 학습할 수 있습니다. 긴 캡션으로 학습할 경우 지정해 주세요.
    デフォルトは75です。`150` または `225` を指定することでトークン長を拡張して学習できます。長いキャプションで学習する場合に指定してください。
    
    단, 학습시의 토큰 확장의 사양은 Automatic1111님의 Web UI와 약간 다르므로(분할의 사양 등), 필요하지 않다면 75로 학습하는 것을 권장합니다.
    ただし学習時のトークン拡張の仕様は Automatic1111 氏のWeb UIとは微妙に異なるため（分割の仕様など）、必要なければ75で学習することをお勧めします。

    clip_skip과 마찬가지로, 모델의 학습 상태와 다른 길이로 학습하려면, 일정 수의 학습 데이터, 긴 학습 시간이 필요할 것입니다.
    clip_skipと同様に、モデルの学習状態と異なる長さで学習するには、ある程度の教師データ枚数、長めの学習時間が必要になると思われます。

- `--weighted_captions`

    지정하면 Automatic1111님의 Web UI와 같은 가중치 캡션을 사용할 수 있습니다. "Textual Inversion and XTI" 이외의 학습에 사용할 수 있습니다. 캡션 뿐만 아니라 DreamBooth 방식의 token string도 사용 가능합니다.
    指定するとAutomatic1111氏のWeb UIと同様の重み付きキャプションが有効になります。「Textual Inversion と XTI」以外の学習に使用できます。キャプションだけでなく DreamBooth 手法の token string でも有効です。

    가중치 캡션의 기록은 Web UI와 거의 같으며, (abc)나 [abc], (abc:1.23) 등이 사용 가능합니다. 중첩도 가능합니다. 괄호 안에 콤마를 포함하면 프롬프트의 셔플/드롭아웃에서 괄호의 대응이 이상해지므로, 괄호 안에는 콤마를 포함하지 말아 주세요.
    重みづけキャプションの記法はWeb UIとほぼ同じで、(abc)や[abc]、(abc:1.23)などが使用できます。入れ子も可能です。括弧内にカンマを含めるとプロンプトのshuffle/dropoutで括弧の対応付けがおかしくなるため、括弧内にはカンマを含めないでください。

- `--persistent_data_loader_workers`

    Windows 환경에서 지정하면 에포크 간의 대기 시간이 크게 단축됩니다.
    Windows環境で指定するとエポック間の待ち時間が大幅に短縮されます。

- `--max_data_loader_n_workers`

    데이터 읽기 프로세스 수를 지정합니다. 프로세스 수가 많으면 데이터 읽기가 빨라지고 GPU를 효율적으로 사용할 수 있지만, 메인 메모리를 소비합니다. 디폴트는 "`8` 또는 `CPU 동시 실행 스레드 수-1`의 작은 값"이므로, 메인 메모리에 여유가 없거나, GPU 사용률이 90% 정도를 넘는 경우, 그 수치를 보면서 `2` 또는 `1` 정도로 줄여 주세요.
    データ読み込みのプロセス数を指定します。プロセス数が多いとデータ読み込みが速くなりGPUを効率的に利用できますが、メインメモリを消費します。デフォルトは「`8` または `CPU同時実行スレッド数-1` の小さいほう」なので、メインメモリに余裕がない場合や、GPU使用率が90%程度以上なら、それらの数値を見ながら `2` または `1` 程度まで下げてください。

- `--logging_dir` / `--log_prefix`
    학습 로그를 저장하는 옵션입니다. logging_dir 옵션에 로그 저장 폴더를 지정해 주세요. TensorBoard 형식의 로그가 저장됩니다.
    学習ログの保存に関するオプションです。logging_dirオプションにログ保存先フォルダを指定してください。TensorBoard形式のログが保存されます。

    예를 들어 `--logging_dir=logs`와 같이 지정하면, 작업 폴더에 logs 폴더가 생성되고, 그 안의 날짜 폴더에 로그가 저장됩니다.
    또한 `--log_prefix` 옵션을 지정하면, 날짜 앞에 지정한 문자열이 추가됩니다. `--logging_dir=logs --log_prefix=db_style1_` 등과 지정하여 식별용으로 사용해 주세요.

    たとえば--logging_dir=logsと指定すると、作業フォルダにlogsフォルダが作成され、その中の日時フォルダにログが保存されます。
    また--log_prefixオプションを指定すると、日時の前に指定した文字列が追加されます。「--logging_dir=logs --log_prefix=db_style1_」などとして識別用にお使いください。

    TensorBoard에서 로그를 확인하려면, 다른 명령 프롬프트를 열고, 작업 폴더에서 다음과 같이 입력합니다.
    TensorBoardでログを確認するには、別のコマンドプロンプトを開き、作業フォルダで以下のように入力します。

    ```
    tensorboard --logdir=logs
    ```

    (tensorboard는 환경 구축 시에 맞춰 설치되므로, 설치되어 있지 않다면 `pip install tensorboard`로 설치해 주세요.)
    （tensorboardは環境整備時にあわせてインストールされると思いますが、もし入っていないなら `pip install tensorboard` で入れてください。）

    그 후 브라우저를 열고, http://localhost:6006/ 에 접속하면 표시됩니다.
    その後ブラウザを開き、http://localhost:6006/ へアクセスすると表示されます。

- `--log_with` / `--log_tracker_name`

    학습 로그의 보존에 관한 옵션입니다. `tensorboard`만이 아닌 `wandb`에도 저장할 수 있습니다. 자세한 것은 링크를 참조해 주세요.
    学習ログの保存に関するオプションです。`tensorboard` だけでなく `wandb`への保存が可能です。詳細は [PR#428](https://github.com/kohya-ss/sd-scripts/pull/428)をご覧ください。

- `--noise_offset`
    이쪽 글의 구현입니다: https://www.crosslabs.org//blog/diffusion-with-offset-noise
    こちらの記事の実装になります: https://www.crosslabs.org//blog/diffusion-with-offset-noise
    전체적으로 어두운, 밝은 이미지의 생성 결과가 좋아질 수 있습니다. LoRA 학습에서도 유효한 것 같습니다. `0.1` 정도의 값을 지정하면 좋을 것 같습니다.
    全体的に暗い、明るい画像の生成結果が良くなる可能性があるようです。LoRA学習でも有効なようです。`0.1` 程度の値を指定するとよいようです。

- `--adaptive_noise_scale` （実験的オプション）(실험적 옵션)
    Noise offset의 값을, latents의 각 채널의 평균값의 절대값에 따라 자동 조정하는 옵션입니다. `--noise_offset`와 함께 지정하면 유효합니다. Noise offset의 값은 `noise_offset + abs(mean(latents, dim=(2,3))) * adaptive_noise_scale`로 계산됩니다. latent는 정규 분포에 가깝기 때문에 noise_offset의 1/10~정도의 값을 지정하면 좋을 것 같습니다.
    Noise offsetの値を、latentsの各チャネルの平均値の絶対値に応じて自動調整するオプションです。`--noise_offset` と同時に指定することで有効になります。Noise offsetの値は `noise_offset + abs(mean(latents, dim=(2,3))) * adaptive_noise_scale` で計算されます。latentは正規分布に近いためnoise_offsetの1/10～同程度の値を指定するとよいかもしれません。

    음수의 값도 지정할 수 있으며, 그 경우 noise offset은 0 이상으로 clip됩니다.
    負の値も指定でき、その場合はnoise offsetは0以上にclipされます。

- `--multires_noise_iterations` / `--multires_noise_discount`
    
    Multi resolution noise (pyramid noise)의 설정입니다. 자세한 것은 [PR#471] 및 이 페이지 [Multi-Resolution Noise for Diffusion Model Training](https://wandb.ai/johnowhitaker/multires_noise/reports/Multi-Resolution-Noise-for-Diffusion-Model-Training--VmlldzozNjYyOTU2)를 참조해 주세요.
    
    `--multires_noise_iterations`에 수치를 지정하면 유효합니다. 6~10 정도의 값이 좋을 것 같습니다. `--multires_noise_discount`에 0.1~0.3 정도의 값(LoRA 학습 등 비교적 데이터셋이 작은 경우 PR 저자의 추천), 또는 0.8 정도의 값(원 글의 추천)을 지정해 주세요(디폴트는 0.3).

    Multi resolution noise (pyramid noise)の設定です。詳細は [PR#471](https://github.com/kohya-ss/sd-scripts/pull/471) およびこちらのページ [Multi-Resolution Noise for Diffusion Model Training](https://wandb.ai/johnowhitaker/multires_noise/reports/Multi-Resolution-Noise-for-Diffusion-Model-Training--VmlldzozNjYyOTU2) を参照してください。
    
    `--multires_noise_iterations` に数値を指定すると有効になります。6~10程度の値が良いようです。`--multires_noise_discount` に0.1~0.3 程度の値（LoRA学習等比較的データセットが小さい場合のPR作者の推奨）、ないしは0.8程度の値（元記事の推奨）を指定してください（デフォルトは 0.3）。

- `--debug_dataset`

    이 옵션을 지정하면, 학습을 하기 전에 어떤 이미지 데이터, 캡션을 학습시킬지 확인할 수 있습니다. Esc키를 누르면 종료하고, `S` 키로 다음 스텝(배치), `E` 키로 다음 에포크로 진행합니다.
    Linux 및 Colab 환경에서는 이미지가 표시되지 않습니다.
    このオプションを付けることで学習を行う前に事前にどのような画像データ、キャプションで学習されるかを確認できます。Escキーを押すと終了してコマンドラインに戻ります。`S`キーで次のステップ（バッチ）、`E`キーで次のエポックに進みます。

    ※Linux環境（Colabを含む）では画像は表示されません。

- `--vae`

    VaE옵션에 Stable Diffusion의 checkpoint, VaE의 checkpoint 파일, Diffusers의 모델 또는 VaE(둘 다 로컬 또는 Hugging Face의 모델 ID가 지정 가능) 중 하나를 지정하면, 그 VaE를 사용하여 학습합니다(캐시할 때나 학습 중의 latents 획득 시).
    vaeオプションにStable Diffusionのcheckpoint、VAEのcheckpointファイル、DiffusesのモデルまたはVAE（ともにローカルまたはHugging FaceのモデルIDが指定できます）のいずれかを指定すると、そのVAEを使って学習します（latentsのキャッシュ時または学習中のlatents取得時）。

    DreamBooth 및 fine tuning에서 저장되는 모델은 이 VaE를 포함한 것이 됩니다.
    DreamBoothおよびfine tuningでは、保存されるモデルはこのVAEを組み込んだものになります。

- `--cache_latents` / `--cache_latents_to_disk`

    사용 VRAM을 줄이기 위해 VAE의 출력을 메인 메모리에 캐시합니다. `flip_aug` 이외의 augmentation은 사용할 수 없어집니다. 또한 전체적인 학습 속도가 약간 빨라집니다.
    使用VRAMを減らすためVAEの出力をメインメモリにキャッシュします。`flip_aug` 以外のaugmentationは使えなくなります。また全体の学習速度が若干速くなります。

    `--cache_latents_to_disk`를 지정하면 캐시를 디스크에 저장합니다. 스크립트를 종료하고 다시 시작해도 캐시가 유효합니다.
    cache_latents_to_diskを指定するとキャッシュをディスクに保存します。スクリプトを終了し、再度起動した場合もキャッシュが有効になります。

- `--min_snr_gamma`

    Min-SNR Weighting 전략을 지정합니다. 자세한 것은 [PR#308]을 참조해 주세요. 논문에서는 `5`가 추천되고 있습니다.
    Min-SNR Weighting strategyを指定します。詳細は[こちら](https://github.com/kohya-ss/sd-scripts/pull/308)を参照してください。論文では`5`が推奨されています。

## 모델 저장에 관한 설정
## モデルの保存に関する設定

- `--save_precision`
    보존시의 데이터 정밀도를 지정합니다. save_precision 옵션에 float, fp16, bf16 중 하나를 지정하면, 그 형식으로 모델을 저장합니다(DreamBooth, fine tuning에서 Diffusers 형식으로 모델을 저장하는 경우는 무효입니다). 모델의 크기를 줄이고 싶을 때 등에 사용해 주세요.
    保存時のデータ精度を指定します。save_precisionオプションにfloat、fp16、bf16のいずれかを指定すると、その形式でモデルを保存します（DreamBooth、fine tuningでDiffusers形式でモデルを保存する場合は無効です）。モデルのサイズを削減したい場合などにお使いください。

- `--save_every_n_epochs` / `--save_state` / `--resume`

    save_every_n_epochs 옵션에 수치를 지정하면, 그 에포크마다 학습 중의 모델을 저장합니다.
    save_every_n_epochsオプションに数値を指定すると、そのエポックごとに学習途中のモデルを保存します。

    save_state 옵션을 함께 지정하면, optimizer 등의 상태도 포함한 학습 상태를 함께 저장합니다(저장한 모델로부터도 학습을 재개할 수 있지만, 그에 비해 정확도의 향상, 학습 시간의 단축이 기대됩니다). 저장 위치는 폴더가 됩니다.
    save_stateオプションを同時に指定すると、optimizer等の状態も含めた学習状態を合わせて保存します（保存したモデルからも学習再開できますが、それに比べると精度の向上、学習時間の短縮が期待できます）。保存先はフォルダになります。
    
    학습상태는 저장 위치 폴더에 `<output_name>-??????-state`(`??????`은 에포크 수)라는 이름의 폴더로 출력됩니다. 장시간 학습 시에 사용해 주세요.
    学習状態は保存先フォルダに `<output_name>-??????-state`（??????はエポック数）という名前のフォルダで出力されます。長時間にわたる学習時にご利用ください。

    보존된 학습 상태로부터 학습을 재개하려면 resume 옵션을 사용합니다. 학습 상태의 폴더(`output_dir`가 아니라 그 안의 state 폴더)를 지정해 주세요.
    保存された学習状態から学習を再開するにはresumeオプションを使います。学習状態のフォルダ（`output_dir` ではなくその中のstateのフォルダ）を指定してください。

    다만 Accelerator의 사양에 따라, 에포크 수, global step은 저장되지 않으며, resume했을 때에도 1부터가 됩니다만 양해해 주세요.
    なおAcceleratorの仕様により、エポック数、global stepは保存されておらず、resumeしたときにも1からになりますがご容赦ください。

- `--save_every_n_steps`

    save_every_n_steps 옵션에 수치를 지정하면, 그 스텝마다 학습 중의 모델을 저장합니다. save_every_n_epochs와 함께 지정할 수 있습니다.
    save_every_n_stepsオプションに数値を指定すると、そのステップごとに学習途中のモデルを保存します。save_every_n_epochsと同時に指定できます。

- `--save_model_as` （DreamBooth, fine tuning のみ）

    모델의 저장 형식을 `ckpt, safetensors, diffusers, diffusers_safetensors` 중에서 선택할 수 있습니다.
    
    `--save_model_as=safetensors`와 같이 지정합니다. Stable Diffusion 형식(ckpt 또는 safetensors)을 읽어 Diffusers 형식으로 저장하는 경우, 부족한 정보는 Hugging Face에서 v1.5 또는 v2.1의 정보를 가져와 보완합니다.

    モデルの保存形式を`ckpt, safetensors, diffusers, diffusers_safetensors` から選べます。
    
    `--save_model_as=safetensors` のように指定します。Stable Diffusion形式（ckptまたはsafetensors）を読み込み、Diffusers形式で保存する場合、不足する情報はHugging Faceからv1.5またはv2.1の情報を落としてきて補完します。

- `--huggingface_repo_id` 等

    huggingface_repo_id가 지정되었을 때, 모델 저장 시에 동시에 HuggingFace에 업로드합니다. 액세스 토큰의 취급에 주의해 주세요(HuggingFace의 문서를 참조해 주세요).

    huggingface_repo_idが指定されているとモデル保存時に同時にHuggingFaceにアップロードします。アクセストークンの取り扱いに注意してください（HuggingFaceのドキュメントを参照してください）。

    다음 예시를 참고해 주세요.
    他の引数をたとえば以下のように指定してください。

    -   `--huggingface_repo_id "your-hf-name/your-model" --huggingface_path_in_repo "path" --huggingface_repo_type model --huggingface_repo_visibility private --huggingface_token hf_YourAccessTokenHere`

    hugghingface_repo_visibility에 `public`을 지정하면 리포지토리가 공개됩니다. 생략하거나 `private`（public以外）를 지정하면 비공개가 됩니다.

    `--save_state` 옵션 지정 시에 `--save_state_to_huggingface`를 지정하면 state도 업로드합니다.

    `--resume` 옵션 지정 시에 `--resume_from_huggingface`를 지정하면 HuggingFace로부터 state를 다운로드하여 재개합니다. 그 때의 --resume 옵션은 `--resume {repo_id}/{path_in_repo}:{revision}:{repo_type}`가 됩니다.
    
    huggingface_repo_visibilityに`public`を指定するとリポジトリが公開されます。省略時または`private`（などpublic以外）を指定すると非公開になります。

    `--save_state`オプション指定時に`--save_state_to_huggingface`を指定するとstateもアップロードします。

    `--resume`オプション指定時に`--resume_from_huggingface`を指定するとHuggingFaceからstateをダウンロードして再開します。その時の --resumeオプションは `--resume {repo_id}/{path_in_repo}:{revision}:{repo_type}`になります。
    
    예시 : `--resume_from_huggingface --resume your-hf-name/your-model/path/test-000002-state:main:model`

    `--async_upload` 옵션을 지정하면 업로드를 비동기로 수행합니다.

    例: `--resume_from_huggingface --resume your-hf-name/your-model/path/test-000002-state:main:model`

    `--async_upload`オプションを指定するとアップロードを非同期で行います。

## 학습에 관한 설정
## オプティマイザ関係

- `--optimizer_type`
    -- 옵티마이저의 종류를 지정합니다. 다음을 지정할 수 있습니다.
    --オプティマイザの種類を指定します。以下が指定できます。 
    - AdamW : [torch.optim.AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)
    - 過去のバージョンのオプション未指定時と同じ #과거 버전의 옵션 미지정 시와 같음
    - AdamW8bit : 引数は同上 #위의 것과 같음, 8비트 버전
    - PagedAdamW8bit : 引数は同上 #위의 것과 같음, 8비트 버전
    - 過去のバージョンの--use_8bit_adam指定時と同じ #과거 버전의 --use_8bit_adam 지정 시와 같음
    - Lion : https://github.com/lucidrains/lion-pytorch 
    - 過去のバージョンの--use_lion_optimizer指定時と同じ #과거 버전의 --use_lion_optimizer 지정 시와 같음
    - Lion8bit : 引数は同上 #위의 것과 같음, 8비트 버전
    - PagedLion8bit : 引数は同上 #위의 것과 같음, 8비트 버전
    - SGDNesterov : [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html), nesterov=True
    - SGDNesterov8bit : 引数は同上 #위의 것과 같음, 8비트 버전
    - DAdaptation(DAdaptAdamPreprint) : https://github.com/facebookresearch/dadaptation
    - DAdaptAdam : 引数は同上 #위의 것과 같음
    - DAdaptAdaGrad : 引数は同上 #위의 것과 같음
    - DAdaptAdan : 引数は同上 #위의 것과 같음
    - DAdaptAdanIP : 引数は同上 #위의 것과 같음
    - DAdaptLion : 引数は同上 #위의 것과 같음
    - DAdaptSGD : 引数は同上 #위의 것과 같음
    - Prodigy : https://github.com/konstmish/prodigy
    - AdaFactor : [Transformers AdaFactor](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules)
    - 任意のオプティマイザ # 어쩌면 직접 구현하는 옵티마이저

- `--learning_rate`
    학습률을 지정합니다. 적절한 학습률은 학습 스크립트에 따라 다릅니다. 각각의 설명을 참조해 주세요.
    学習率を指定します。適切な学習率は学習スクリプトにより異なりますので、それぞれの説明を参照してください。

- `--lr_scheduler` / `--lr_warmup_steps` / `--lr_scheduler_num_cycles` / `--lr_scheduler_power`
  
    학습률 스케줄러 관련의 지정입니다.
    学習率のスケジューラ関連の指定です。

    lr_scheduler 옵션으로 학습률의 스케줄러를 linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup, 임의의 스케줄러에서 선택할 수 있습니다. 디폴트는 constant입니다.
    lr_schedulerオプションで学習率のスケジューラをlinear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup, 任意のスケジューラから選べます。デフォルトはconstantです。
    
    lr_warmup_steps 옵션으로 스케줄러의 웜업(점점 학습률을 바꾸어 가는) 스텝 수를 지정할 수 있습니다.
    lr_warmup_stepsでスケジューラのウォームアップ（だんだん学習率を変えていく）ステップ数を指定できます。
    
    lr_scheduler_num_cycles 옵션으로 cosine_with_restarts 스케줄러의 리스타트 횟수, lr_scheduler_power 옵션으로 polynomial 스케줄러의 polynomial power를 지정할 수 있습니다.
    lr_scheduler_num_cycles は cosine with restartsスケジューラでのリスタート回数、lr_scheduler_power は polynomialスケジューラでのpolynomial power です。

    상세는 각자 알아서 찾아보세요.
    詳細については各自お調べください。

    직접 구현한 스케쥴러를 사용하고자 할 경우, `--scheduler_args`로 직접 옵션을 지정해주세요.
    任意のスケジューラを使う場合、任意のオプティマイザと同様に、`--scheduler_args`でオプション引数を指定してください。

### 옵티마이저의 지정에 관하여
### オプティマイザの指定について

옵티마이저의 argument는 --optimizer_args 옵션으로 지정합니다. key=value의 형식으로, 여러 개의 값을 지정할 수 있습니다. 또한 value는 콤마로 구분하여 여러 개의 값을 지정할 수 있습니다. 예를 들어 AdamW 옵티마이저에 argument를 지정하고 싶은 경우, `--optimizer_args weight_decay=0.01 betas=.9,.999`와 같이 지정합니다.
オプティマイザのオプション引数は--optimizer_argsオプションで指定してください。key=valueの形式で、複数の値が指定できます。また、valueはカンマ区切りで複数の値が指定できます。たとえばAdamWオプティマイザに引数を指定する場合は、``--optimizer_args weight_decay=0.01 betas=.9,.999``のようになります。

옵션의 지정에 관해서는 각 옵티마이저의 스크립트를 참조해 주세요.
オプション引数を指定する場合は、それぞれのオプティマイザの仕様をご確認ください。

일부 옵티마이저의 경우 필수의 argument가 있으며, 생략하면 자동으로 추가됩니다(SGDNesterov의 momentum 등). 콘솔의 출력을 확인해 주세요.
一部のオプティマイザでは必須の引数があり、省略すると自動的に追加されます（SGDNesterovのmomentumなど）。コンソールの出力を確認してください。

D-Adaptation 옵티마이저는 학습률을 자동으로 조정합니다. 학습률의 옵션에 지정한 값은 학습률 그 자체가 아니라 D-Adaptation이 결정한 학습률의 적용률이 됩니다. 보통은 1.0을 지정해 주세요. Text Encoder에 U-Net의 절반의 학습률을 지정하고 싶은 경우, `--text_encoder_lr=0.5 --unet_lr=1.0`와 같이 지정합니다.
D-Adaptationオプティマイザは学習率を自動調整します。学習率のオプションに指定した値は学習率そのものではなくD-Adaptationが決定した学習率の適用率になりますので、通常は1.0を指定してください。Text EncoderにU-Netの半分の学習率を指定したい場合は、``--text_encoder_lr=0.5 --unet_lr=1.0``と指定します。

AdaFactor 옵티마이저는 relative_step=True를 지정하면 학습률을 자동 조정할 수 있습니다(생략 시 디폴트로 추가됩니다). 자동 조정할 경우 학습률의 스케줄러에는 adafactor_scheduler가 강제로 사용됩니다. 또한 scale_parameter와 warmup_init를 지정하면 좋습니다.
AdaFactorオプティマイザはrelative_step=Trueを指定すると学習率を自動調整できます（省略時はデフォルトで追加されます）。自動調整する場合は学習率のスケジューラにはadafactor_schedulerが強制的に使用されます。またscale_parameterとwarmup_initを指定するとよいようです。

자동조절을 할 경우 옵션은 예를 들면 ``--optimizer_args "relative_step=True" "scale_parameter=True" "warmup_init=True"``의 형식이 됩니다.
自動調整する場合のオプション指定はたとえば ``--optimizer_args "relative_step=True" "scale_parameter=True" "warmup_init=True"`` のようになります。

학습률을 자동 조정하지 않을 경우 옵션 인수 ``relative_step=False``를 추가해 주세요. 그 경우 학습률의 스케줄러에는 constant_with_warmup가, 또한 그라디언트의 clip norm을 하지 않는 것이 권장되고 있습니다. 그러므로 인수는 ``--optimizer_type=adafactor --optimizer_args "relative_step=False" --lr_scheduler="constant_with_warmup" --max_grad_norm=0.0``의 형식이 됩니다.
学習率を自動調整しない場合はオプション引数 ``relative_step=False`` を追加してください。その場合、学習率のスケジューラにはconstant_with_warmupが、また勾配のclip normをしないことが推奨されているようです。そのため引数は ``--optimizer_type=adafactor --optimizer_args "relative_step=False" --lr_scheduler="constant_with_warmup" --max_grad_norm=0.0`` のようになります。

### 커스텀 옵티마이저 사용
### 任意のオプティマイザを使う

``torch.optim``에서 옵티마이저를 사용하려면 클래스명만을(``--optimizer_type=RMSprop`` 등) 다른 모듈의 옵티마이저를 사용하려면 ``モジュール名.クラス名``을 지정해 주세요(``--optimizer_type=bitsandbytes.optim.lamb.LAMB`` 등).
``torch.optim`` のオプティマイザを使う場合にはクラス名のみを（``--optimizer_type=RMSprop``など）、他のモジュールのオプティマイザを使う時は「モジュール名.クラス名」を指定してください（``--optimizer_type=bitsandbytes.optim.lamb.LAMB``など）。

(내부에서 importlib만 하고 있을 뿐 동작은 확인하지 않았습니다. 필요하면 패키지를 설치해 주세요.)
（内部でimportlibしているだけで動作は未確認です。必要ならパッケージをインストールしてください。）


<!-- 
## 커스텀 사이즈 이미지로 학습할 경우 --resolution
## 任意サイズの画像での学習 --resolution
정사각형이 아닌 이미지로 학습할 수 있습니다. resolution에 ``448,640``과 같이 ``가로,세로``로 지정해 주세요. 가로와 세로는 64로 나누어 떨어져야 합니다. 학습용 이미지, 정규화 이미지의 크기를 맞춰 주세요.
正方形以外で学習できます。resolutionに「448,640」のように「幅,高さ」で指定してください。幅と高さは64で割り切れる必要があります。学習用画像、正則化画像のサイズを合わせてください。

개인적으로는 세로로 긴 이미지를 생성하는 경우가 많아 ``448,640``과 같이 세로로 긴 이미지로 학습하는 경우도 있습니다.
個人的には縦長の画像を生成することが多いため「448,640」などで学習することもあります。

## Aspect Ratio Bucketing --enable_bucket / --min_bucket_reso / --max_bucket_reso
enable_bucket 옵션을 지정하면 Aspect Ratio Bucketing이 활성화됩니다. Stable Diffusion은 512x512로 학습되어 있지만, 그 외에도 256x768, 384x640 등의 해상도로도 학습합니다.
enable_bucketオプションを指定すると有効になります。Stable Diffusionは512x512で学習されていますが、それに加えて256x768や384x640といった解像度でも学習します。

이 옵션을 지정할 경우, 학습용 이미지, 정규화 이미지를 특정의 해상도로 통일할 필요는 없습니다. 몇 가지의 해상도（Aspect Ratio）에서 최적의 것을 선택하여 그 해상도로 학습합니다.
해상도는 64픽셀 단위이므로, 원래 이미지와 Aspect Ratio가 완전히 일치하지 않는 경우가 있습니다만, 그 경우는 약간의 트리밍이 됩니다.
このオプションを指定した場合は、学習用画像、正則化画像を特定の解像度に統一する必要はありません。いくつかの解像度（アスペクト比）から最適なものを選び、その解像度で学習します。
解像度は64ピクセル単位のため、元画像とアスペクト比が完全に一致しない場合がありますが、その場合は、はみ出した部分がわずかにトリミングされます。

해상도별 최소 크기는 min_bucket_reso 옵션으로, 최대 크기는 max_bucket_reso로 지정할 수 있습니다. 디폴트는 각각 256, 1024입니다.
예를 들어 최소 크기에 384를 지정하면, 256x1024, 320x768 등의 해상도는 사용하지 않게 됩니다.
해상도를 768x768과 같이 크게 한 경우, 최대 크기에 1280 등을 지정해 주는 것도 좋을 것입니다.
解像度の最小サイズをmin_bucket_resoオプションで、最大サイズをmax_bucket_resoで指定できます。デフォルトはそれぞれ256、1024です。
たとえば最小サイズに384を指定すると、256x1024や320x768などの解像度は使わなくなります。
解像度を768x768のように大きくした場合、最大サイズに1280などを指定しても良いかもしれません。

또한 Aspect Ratio Bucketing을 활성화할 때에는, 정규화 이미지도 학습용 이미지와 비슷한 경향의 다양한 해상도를 준비하는 것이 좋을 것입니다.
なおAspect Ratio Bucketingを有効にするときには、正則化画像についても、学習用画像と似た傾向の様々な解像度を用意した方がいいかもしれません。

 (하나의 배치 내의 이미지가 학습용 이미지, 정규화 이미지에 편중되지 않게 하기 위해서입니다. 큰 영향은 없을 것입니다만…….)
（ひとつのバッチ内の画像が学習用画像、正則化画像に偏らなくなるため。そこまで大きな影響はないと思いますが……。）

## augmentation --color_aug / --flip_aug
augmentation은 학습시에 동적으로 데이터를 변화시켜 모델의 성능을 높이는 방법입니다. color_aug로 색조를 미묘하게 변화시키면서, flip_aug로 좌우 반전을 하면서 학습합니다.
augmentationは学習時に動的にデータを変化させることで、モデルの性能を上げる手法です。color_augで色合いを微妙に変えつつ、flip_augで左右反転をしつつ、学習します。

동적으로 데이터를 변화시켜야 하므로, cache_latents 옵션과 동시에 지정할 수 없습니다.
動的にデータを変化させるため、cache_latentsオプションと同時に指定できません。

## 배정밀도를 fp16으로 한 학습（실험적 기능） --full_fp16
## 勾配をfp16とした学習（実験的機能） --full_fp16
full_fp16 옵션을 지정하면, 배정밀도를 float32에서 float16（fp16）로 변경하여 학습합니다（mixed precision이 아닌 완전한 fp16 학습이 됩니다）.
이렇게 하면 SD1.x의 512x512 사이즈에서는 8GB 미만, SD2.x의 512x512 사이즈에서는 12GB 미만의 VRAM 사용량으로 학습할 수 있습니다.
full_fp16オプションを指定すると勾配を通常のfloat32からfloat16（fp16）に変更して学習します（mixed precisionではなく完全なfp16学習になるようです）。
これによりSD1.xの512x512サイズでは8GB未満、SD2.xの512x512サイズで12GB未満のVRAM使用量で学習できるようです。

또한 acceleration config에서 미리 fp16을 지정하고, 옵션으로 ``mixed_precision="fp16"``을 지정해 주세요（bf16은 동작하지 않습니다）.
あらかじめaccelerate configでfp16を指定し、オプションで ``mixed_precision="fp16"`` としてください（bf16では動作しません）。

메모리 사용량을 최소화하려면, xformers, use_8bit_adam, cache_latents, gradient_checkpointing의 각 옵션을 지정하고, train_batch_size를 1로 지정해 주세요.
メモリ使用量を最小化するためには、xformers、use_8bit_adam、cache_latents、gradient_checkpointingの各オプションを指定し、train_batch_sizeを1としてください。

 (여유가 있다면 train_batch_size를 단계적으로 늘리면 약간의 정확도가 올라갈 것입니다.)
（余裕があるようならtrain_batch_sizeを段階的に増やすと若干精度が上がるはずです。）

PyTorch의 소스에 패치를 적용하여 강제로 구현하고 있습니다（PyTorch 1.12.1과 1.13.0에서 확인）. 정확도는 상당히 떨어지며, 중간에 학습 실패할 확률도 높아집니다.
학습률이나 스텝 수의 설정도 까다로운 것 같습니다. 이를 인지한 상태에서 자기 책임 하에 사용해 주세요.
PyTorchのソースにパッチを当てて無理やり実現しています（PyTorch 1.12.1と1.13.0で確認）。精度はかなり落ちますし、途中で学習失敗する確率も高くなります。
学習率やステップ数の設定もシビアなようです。それらを認識したうえで自己責任でお使いください。

-->

# 메타데이터 파일의 생성
# メタデータファイルの作成

## 교사 데이터 준비 (?)
## 教師データの用意

학습시키고자 하는 이미지 데이터를 준비하고, 임의의 폴더에 넣어 주세요.
前述のように学習させたい画像データを用意し、任意のフォルダに入れてください。

예를 들어 아래와 같이 이미지를 저장합니다.
たとえば以下のように画像を格納します。

![教師データフォルダのスクショ](https://user-images.githubusercontent.com/52813779/208907739-8e89d5fa-6ca8-4b60-8927-f484d2a9ae04.png)

## 자동 캡셔닝
## 自動キャプショニング

캡션을 직접 준비하지 않고, 태그만으로 학습하고자 하는 경우에는 이 절을 건너뛰세요.
キャプションを使わずタグだけで学習する場合はスキップしてください。

또한 수동으로 캡션을 준비하는 경우, 캡션은 교사 데이터 이미지와 같은 폴더에, 같은 파일명, 확장자.caption 등으로 준비해 주세요. 각 파일은 1줄의 텍스트 파일로 합니다.
また手動でキャプションを用意する場合、キャプションは教師データ画像と同じディレクトリに、同じファイル名、拡張子.caption等で用意してください。各ファイルは1行のみのテキストファイルとします。

### BLIP에 의한 캡셔닝
### BLIPによるキャプショニング

최신 버전에서는 BLIP의 다운로드, 가중치의 다운로드, 가상 환경의 추가는 필요하지 않습니다. 그대로 동작합니다.
最新版ではBLIPのダウンロード、重みのダウンロード、仮想環境の追加は不要になりました。そのままで動作します。

finetune 폴더 내의 make_captions.py를 실행합니다.
finetuneフォルダ内のmake_captions.pyを実行します。

```
python finetune\make_captions.py --batch_size <バッチサイズ Batch size> <教師データフォルダ 데이터 폴더>
```
배치 사이즈 8, 교사 데이터를 부모 폴더의 train_data에 두었다면, 아래와 같이 합니다.
バッチサイズ8、教師データを親フォルダのtrain_dataに置いた場合、以下のようになります。

```
python finetune\make_captions.py --batch_size 8 ..\train_data
```

캡션 파일은 교사 데이터 이미지와 같은 폴더에, 같은 파일명, 확장자.caption으로 생성됩니다.
キャプションファイルが教師データ画像と同じディレクトリに、同じファイル名、拡張子.captionで作成されます。

batch_size는 GPU의 VRAM 용량에 따라 적절히 조절해 주세요. 크게 할수록 빠릅니다（VRAM 12GB에서도 조금 더 늘릴 수 있을 것입니다）.
max_length 옵션으로 캡션의 최대 길이를 지정할 수 있습니다. 디폴트는 75입니다. 모델을 토큰 길이 225로 학습할 경우에는 길게 해도 좋을 것입니다.
caption_extension 옵션으로 캡션의 확장자를 변경할 수 있습니다. 디폴트는 .caption입니다（.txt로 하면 아래의 DeepDanbooru와 겹칩니다）.

batch_sizeはGPUのVRAM容量に応じて増減してください。大きいほうが速くなります（VRAM 12GBでももう少し増やせると思います）。
max_lengthオプションでキャプションの最大長を指定できます。デフォルトは75です。モデルをトークン長225で学習する場合には長くしても良いかもしれません。
caption_extensionオプションでキャプションの拡張子を変更できます。デフォルトは.captionです（.txtにすると後述のDeepDanbooruと競合します）。

복수 이미지 폴더가 있는 경우, 각각의 폴더에 대해 실행해 주세요.
複数の教師データフォルダがある場合には、それぞれのフォルダに対して実行してください。

또한, 추론은 랜덤성이 존재하기에, 실행할 때마다 결과가 바뀔 것입니다. 고정하고자 할 경우 --seed 옵션으로 ``--seed 42``와 같이 난수 seed를 지정해 주세요.
なお、推論にランダム性があるため、実行するたびに結果が変わります。固定する場合には--seedオプションで `--seed 42` のように乱数seedを指定してください。

그 외의 경우에는 `--help`로 도움말을 참조해 주세요（파라미터의 의미에 대해서는 문서가 정리되어 있지 않은 것 같아 소스를 보는 수밖에 없습니다）.
その他のオプションは `--help` でヘルプをご参照ください（パラメータの意味についてはドキュメントがまとまっていないようで、ソースを見るしかないようです）。

디폴트로는 교사 데이터 이미지와 같은 폴더에, 같은 파일명, 확장자.caption으로 생성됩니다.
デフォルトでは拡張子.captionでキャプションファイルが生成されます。

![captionが生成されたフォルダ](https://user-images.githubusercontent.com/52813779/208908845-48a9d36c-f6ee-4dae-af71-9ab462d1459e.png)

예를 들어 아래와 같이 캡션 파일이 생성됩니다.
たとえば以下のようなキャプションが付きます。

![キャプションと画像](https://user-images.githubusercontent.com/52813779/208908947-af936957-5d73-4339-b6c8-945a52857373.png)

## DeepDanbooru에 의한 캡셔닝
## DeepDanbooruによるタグ付け

danbooru 태그의 태그 자체를 하지 않을 경우, 캡션과 태그 정보의 전처리로 넘어가 주세요.
danbooruタグのタグ付け自体を行わない場合は「キャプションとタグ情報の前処理」に進んでください。

태그는 WD14 또는 DeepDanbooru로 합니다. WD14Tagger의 경우, 다음 절로 넘어가 주세요.
タグ付けはDeepDanbooruまたはWD14Taggerで行います。WD14Taggerのほうが精度が良いようです。WD14Taggerでタグ付けする場合は、次の章へ進んでください。

### 환경 준비
### 環境整備

DeepDanbooru는 repository를 작업폴더에 클론하거나, zip을 다운로드하여 풀어 주세요. 저는 zip으로 풀었습니다.
또한 DeepDanbooru의 Releases 페이지에서 ``deepdanbooru-v3-20211112-sgd-e28.zip``을 다운로드하여 DeepDanbooru 폴더에 풀어 주세요.

DeepDanbooru https://github.com/KichangKim/DeepDanbooru  を作業フォルダにcloneしてくるか、zipをダウンロードして展開します。私はzipで展開しました。
またDeepDanbooruのReleasesのページ https://github.com/KichangKim/DeepDanbooru/releases  の「DeepDanbooru Pretrained Model v3-20211112-sgd-e28」のAssetsから、deepdanbooru-v3-20211112-sgd-e28.zipをダウンロードしてきてDeepDanbooruのフォルダに展開します。

以下からダウンロードします。Assetsをクリックして開き、そこからダウンロードします。

![DeepDanbooruダウンロードページ](https://user-images.githubusercontent.com/52813779/208909417-10e597df-7085-41ee-bd06-3e856a1339df.png)

以下のようなこういうディレクトリ構造にしてください

![DeepDanbooruのディレクトリ構造](https://user-images.githubusercontent.com/52813779/208909486-38935d8b-8dc6-43f1-84d3-fef99bc471aa.png)
(번역 건너뜀)
Diffusersの環境に必要なライブラリをインストールします。DeepDanbooruのフォルダに移動してインストールします（実質的にはtensorflow-ioが追加されるだけだと思います）。

```
pip install -r requirements.txt
```

続いてDeepDanbooru自体をインストールします。

```
pip install .
```

以上でタグ付けの環境整備は完了です。

### タグ付けの実施
DeepDanbooruのフォルダに移動し、deepdanbooruを実行してタグ付けを行います。

```
deepdanbooru evaluate <教師データフォルダ> --project-path deepdanbooru-v3-20211112-sgd-e28 --allow-folder --save-txt
```

教師データを親フォルダのtrain_dataに置いた場合、以下のようになります。

```
deepdanbooru evaluate ../train_data --project-path deepdanbooru-v3-20211112-sgd-e28 --allow-folder --save-txt
```

タグファイルが教師データ画像と同じディレクトリに、同じファイル名、拡張子.txtで作成されます。1件ずつ処理されるためわりと遅いです。

複数の教師データフォルダがある場合には、それぞれのフォルダに対して実行してください。

以下のように生成されます。

![DeepDanbooruの生成ファイル](https://user-images.githubusercontent.com/52813779/208909855-d21b9c98-f2d3-4283-8238-5b0e5aad6691.png)

こんな感じにタグが付きます（すごい情報量……）。

![DeepDanbooruタグと画像](https://user-images.githubusercontent.com/52813779/208909908-a7920174-266e-48d5-aaef-940aba709519.png)

## WD14Taggerによるタグ付け

DeepDanbooruの代わりにWD14Taggerを用いる手順です。

Automatic1111氏のWebUIで使用しているtaggerを利用します。こちらのgithubページ（https://github.com/toriato/stable-diffusion-webui-wd14-tagger#mrsmilingwolfs-model-aka-waifu-diffusion-14-tagger ）の情報を参考にさせていただきました。

最初の環境整備で必要なモジュールはインストール済みです。また重みはHugging Faceから自動的にダウンロードしてきます。

### タグ付けの実施

スクリプトを実行してタグ付けを行います。
```
python tag_images_by_wd14_tagger.py --batch_size <バッチサイズ> <教師データフォルダ>
```

教師データを親フォルダのtrain_dataに置いた場合、以下のようになります。
```
python tag_images_by_wd14_tagger.py --batch_size 4 ..\train_data
```

初回起動時にはモデルファイルがwd14_tagger_modelフォルダに自動的にダウンロードされます（フォルダはオプションで変えられます）。以下のようになります。

![ダウンロードされたファイル](https://user-images.githubusercontent.com/52813779/208910447-f7eb0582-90d6-49d3-a666-2b508c7d1842.png)

タグファイルが教師データ画像と同じディレクトリに、同じファイル名、拡張子.txtで作成されます。

![生成されたタグファイル](https://user-images.githubusercontent.com/52813779/208910534-ea514373-1185-4b7d-9ae3-61eb50bc294e.png)

![タグと画像](https://user-images.githubusercontent.com/52813779/208910599-29070c15-7639-474f-b3e4-06bd5a3df29e.png)

threshオプションで、判定されたタグのconfidence（確信度）がいくつ以上でタグをつけるかが指定できます。デフォルトはWD14Taggerのサンプルと同じ0.35です。値を下げるとより多くのタグが付与されますが、精度は下がります。

batch_sizeはGPUのVRAM容量に応じて増減してください。大きいほうが速くなります（VRAM 12GBでももう少し増やせると思います）。caption_extensionオプションでタグファイルの拡張子を変更できます。デフォルトは.txtです。

model_dirオプションでモデルの保存先フォルダを指定できます。

またforce_downloadオプションを指定すると保存先フォルダがあってもモデルを再ダウンロードします。

複数の教師データフォルダがある場合には、それぞれのフォルダに対して実行してください。

## キャプションとタグ情報の前処理

スクリプトから処理しやすいようにキャプションとタグをメタデータとしてひとつのファイルにまとめます。

### キャプションの前処理

キャプションをメタデータに入れるには、作業フォルダ内で以下を実行してください（キャプションを学習に使わない場合は実行不要です）（実際は1行で記述します、以下同様）。`--full_path` オプションを指定してメタデータに画像ファイルの場所をフルパスで格納します。このオプションを省略すると相対パスで記録されますが、フォルダ指定が `.toml` ファイル内で別途必要になります。

```
python merge_captions_to_metadata.py --full_path <教師データフォルダ>
　  --in_json <読み込むメタデータファイル名> <メタデータファイル名>
```

メタデータファイル名は任意の名前です。
教師データがtrain_data、読み込むメタデータファイルなし、メタデータファイルがmeta_cap.jsonの場合、以下のようになります。

```
python merge_captions_to_metadata.py --full_path train_data meta_cap.json
```

caption_extensionオプションでキャプションの拡張子を指定できます。

複数の教師データフォルダがある場合には、full_path引数を指定しつつ、それぞれのフォルダに対して実行してください。

```
python merge_captions_to_metadata.py --full_path 
    train_data1 meta_cap1.json
python merge_captions_to_metadata.py --full_path --in_json meta_cap1.json 
    train_data2 meta_cap2.json
```

in_jsonを省略すると書き込み先メタデータファイルがあるとそこから読み込み、そこに上書きします。

__※in_jsonオプションと書き込み先を都度書き換えて、別のメタデータファイルへ書き出すようにすると安全です。__

### タグの前処理

同様にタグもメタデータにまとめます（タグを学習に使わない場合は実行不要です）。
```
python merge_dd_tags_to_metadata.py --full_path <教師データフォルダ> 
    --in_json <読み込むメタデータファイル名> <書き込むメタデータファイル名>
```

先と同じディレクトリ構成で、meta_cap.jsonを読み、meta_cap_dd.jsonに書きだす場合、以下となります。
```
python merge_dd_tags_to_metadata.py --full_path train_data --in_json meta_cap.json meta_cap_dd.json
```

複数の教師データフォルダがある場合には、full_path引数を指定しつつ、それぞれのフォルダに対して実行してください。

```
python merge_dd_tags_to_metadata.py --full_path --in_json meta_cap2.json
    train_data1 meta_cap_dd1.json
python merge_dd_tags_to_metadata.py --full_path --in_json meta_cap_dd1.json 
    train_data2 meta_cap_dd2.json
```

in_jsonを省略すると書き込み先メタデータファイルがあるとそこから読み込み、そこに上書きします。

__※in_jsonオプションと書き込み先を都度書き換えて、別のメタデータファイルへ書き出すようにすると安全です。__

### キャプションとタグのクリーニング

ここまででメタデータファイルにキャプションとDeepDanbooruのタグがまとめられています。ただ自動キャプショニングにしたキャプションは表記ゆれなどがあり微妙（※）ですし、タグにはアンダースコアが含まれていたりratingが付いていたりしますので（DeepDanbooruの場合）、エディタの置換機能などを用いてキャプションとタグのクリーニングをしたほうがいいでしょう。

※たとえばアニメ絵の少女を学習する場合、キャプションにはgirl/girls/woman/womenなどのばらつきがあります。また「anime girl」なども単に「girl」としたほうが適切かもしれません。

クリーニング用のスクリプトが用意してありますので、スクリプトの内容を状況に応じて編集してお使いください。

（教師データフォルダの指定は不要になりました。メタデータ内の全データをクリーニングします。）

```
python clean_captions_and_tags.py <読み込むメタデータファイル名> <書き込むメタデータファイル名>
```

--in_jsonは付きませんのでご注意ください。たとえば次のようになります。

```
python clean_captions_and_tags.py meta_cap_dd.json meta_clean.json
```

以上でキャプションとタグの前処理は完了です。

## latentsの事前取得

※ このステップは必須ではありません。省略しても学習時にlatentsを取得しながら学習できます。
また学習時に `random_crop` や `color_aug` などを行う場合にはlatentsの事前取得はできません（画像を毎回変えながら学習するため）。事前取得をしない場合、ここまでのメタデータで学習できます。

あらかじめ画像の潜在表現を取得しディスクに保存しておきます。それにより、学習を高速に進めることができます。あわせてbucketing（教師データをアスペクト比に応じて分類する）を行います。

作業フォルダで以下のように入力してください。
```
python prepare_buckets_latents.py --full_path <教師データフォルダ>  
    <読み込むメタデータファイル名> <書き込むメタデータファイル名> 
    <fine tuningするモデル名またはcheckpoint> 
    --batch_size <バッチサイズ> 
    --max_resolution <解像度 幅,高さ> 
    --mixed_precision <精度>
```

モデルがmodel.ckpt、バッチサイズ4、学習解像度は512\*512、精度no（float32）で、meta_clean.jsonからメタデータを読み込み、meta_lat.jsonに書き込む場合、以下のようになります。

```
python prepare_buckets_latents.py --full_path 
    train_data meta_clean.json meta_lat.json model.ckpt 
    --batch_size 4 --max_resolution 512,512 --mixed_precision no
```

教師データフォルダにnumpyのnpz形式でlatentsが保存されます。

解像度の最小サイズを--min_bucket_resoオプションで、最大サイズを--max_bucket_resoで指定できます。デフォルトはそれぞれ256、1024です。たとえば最小サイズに384を指定すると、256\*1024や320\*768などの解像度は使わなくなります。
解像度を768\*768のように大きくした場合、最大サイズに1280などを指定すると良いでしょう。

--flip_augオプションを指定すると左右反転のaugmentation（データ拡張）を行います。疑似的にデータ量を二倍に増やすことができますが、データが左右対称でない場合に指定すると（例えばキャラクタの外見、髪型など）学習がうまく行かなくなります。


（反転した画像についてもlatentsを取得し、\*\_flip.npzファイルを保存する単純な実装です。fline_tune.pyには特にオプション指定は必要ありません。\_flip付きのファイルがある場合、flip付き・なしのファイルを、ランダムに読み込みます。）

バッチサイズはVRAM 12GBでももう少し増やせるかもしれません。
解像度は64で割り切れる数字で、"幅,高さ"で指定します。解像度はfine tuning時のメモリサイズに直結します。VRAM 12GBでは512,512が限界と思われます（※）。16GBなら512,704や512,768まで上げられるかもしれません。なお256,256等にしてもVRAM 8GBでは厳しいようです（パラメータやoptimizerなどは解像度に関係せず一定のメモリが必要なため）。

※batch size 1の学習で12GB VRAM、640,640で動いたとの報告もありました。

以下のようにbucketingの結果が表示されます。

![bucketingの結果](https://user-images.githubusercontent.com/52813779/208911419-71c00fbb-2ce6-49d5-89b5-b78d7715e441.png)

複数の教師データフォルダがある場合には、full_path引数を指定しつつ、それぞれのフォルダに対して実行してください。
```
python prepare_buckets_latents.py --full_path  
    train_data1 meta_clean.json meta_lat1.json model.ckpt 
    --batch_size 4 --max_resolution 512,512 --mixed_precision no

python prepare_buckets_latents.py --full_path 
    train_data2 meta_lat1.json meta_lat2.json model.ckpt 
    --batch_size 4 --max_resolution 512,512 --mixed_precision no

```
読み込み元と書き込み先を同じにすることも可能ですが別々の方が安全です。

__※引数を都度書き換えて、別のメタデータファイルに書き込むと安全です。__

